{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "4ac6df2f421f4dcfac60ffd52407a843": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_023eb3bc3896450dbe5aa7c7751beb7a",
       "IPY_MODEL_45e880576e1243ff922c2e51e33f9cb5",
       "IPY_MODEL_dbb9b51b0d5e49b2aecf571091162f80"
      ],
      "layout": "IPY_MODEL_b1497a51af1f472e93471b79a430618b"
     }
    },
    "023eb3bc3896450dbe5aa7c7751beb7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_582d5b88d04e4fd5b4bc8b06dfde6e90",
      "placeholder": "​",
      "style": "IPY_MODEL_d60635aaa32b463a933a96b36aabe3a6",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "45e880576e1243ff922c2e51e33f9cb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f542a31fc8847a3bf408ff1de8af112",
      "max": 2284,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e2462efd37374e4a8520dfad584bb970",
      "value": 2284
     }
    },
    "dbb9b51b0d5e49b2aecf571091162f80": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f370af5a686046e99fea5246e7b40317",
      "placeholder": "​",
      "style": "IPY_MODEL_43015ad798d8483690c854411ebdfa80",
      "value": " 2.28k/2.28k [00:00&lt;00:00, 222kB/s]"
     }
    },
    "b1497a51af1f472e93471b79a430618b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "582d5b88d04e4fd5b4bc8b06dfde6e90": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d60635aaa32b463a933a96b36aabe3a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1f542a31fc8847a3bf408ff1de8af112": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2462efd37374e4a8520dfad584bb970": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f370af5a686046e99fea5246e7b40317": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43015ad798d8483690c854411ebdfa80": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1226e5154dd4c8591310fbaa8b0a9fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4d564dcb7de4c498fbe842aae305f0d",
       "IPY_MODEL_34e0050072754f128271fc0819b30328",
       "IPY_MODEL_eb2a0adbeccd4536ae316d3b37757dd4"
      ],
      "layout": "IPY_MODEL_378a372042fe4d3696b08a647b4967d2"
     }
    },
    "b4d564dcb7de4c498fbe842aae305f0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30c9cd3caf4d4b4bb79e65744ba3601a",
      "placeholder": "​",
      "style": "IPY_MODEL_0b05bf0430904f148fa55b3118533e57",
      "value": "tokenizer.model: 100%"
     }
    },
    "34e0050072754f128271fc0819b30328": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd6aab2782624c1c8220512f7324b016",
      "max": 499723,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1bcf5a22fe9242168a364b1a6b2fa337",
      "value": 499723
     }
    },
    "eb2a0adbeccd4536ae316d3b37757dd4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ecf28296e0346a4af6da53aa73b8579",
      "placeholder": "​",
      "style": "IPY_MODEL_aa558b3efff64710ad70f581d62bd269",
      "value": " 500k/500k [00:00&lt;00:00, 20.9MB/s]"
     }
    },
    "378a372042fe4d3696b08a647b4967d2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30c9cd3caf4d4b4bb79e65744ba3601a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b05bf0430904f148fa55b3118533e57": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dd6aab2782624c1c8220512f7324b016": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1bcf5a22fe9242168a364b1a6b2fa337": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2ecf28296e0346a4af6da53aa73b8579": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa558b3efff64710ad70f581d62bd269": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e07aac9228914b499ae718db3f9ba789": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d5446e9a0ff24fcf868ad67d37e53f68",
       "IPY_MODEL_38b3f9f2df97419c872fb3ff878735ce",
       "IPY_MODEL_e1c34849111f4be084c83eea3230d553"
      ],
      "layout": "IPY_MODEL_e0f07bf1fb5f44dea503b48c64c8b81c"
     }
    },
    "d5446e9a0ff24fcf868ad67d37e53f68": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29f0f81e20a24fb4aee94a1256b8f6b6",
      "placeholder": "​",
      "style": "IPY_MODEL_e19a002be9d44e8988e268b82414739b",
      "value": "tokenizer.json: 100%"
     }
    },
    "38b3f9f2df97419c872fb3ff878735ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b21d61300c8437d84ff3a2184067278",
      "max": 1842665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9284a4cad25249248a93b13087c9592c",
      "value": 1842665
     }
    },
    "e1c34849111f4be084c83eea3230d553": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_324c045b882b4eab929cad294083c6da",
      "placeholder": "​",
      "style": "IPY_MODEL_d7f42e43ec7449278b0ff59b483064a3",
      "value": " 1.84M/1.84M [00:00&lt;00:00, 2.84MB/s]"
     }
    },
    "e0f07bf1fb5f44dea503b48c64c8b81c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29f0f81e20a24fb4aee94a1256b8f6b6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e19a002be9d44e8988e268b82414739b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b21d61300c8437d84ff3a2184067278": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9284a4cad25249248a93b13087c9592c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "324c045b882b4eab929cad294083c6da": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7f42e43ec7449278b0ff59b483064a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "304da90c6f18498981684233abc064b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5da053d61e0e41b0a85d5503f54cc25a",
       "IPY_MODEL_dbfdf912175441cc80e18b47640fa3a3",
       "IPY_MODEL_ae64f835eee14782af7811b4e157ca95"
      ],
      "layout": "IPY_MODEL_c4b51892aa5741c0bdc1ae2b2f649647"
     }
    },
    "5da053d61e0e41b0a85d5503f54cc25a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6411565dbfb541eabaa6090e77c74a32",
      "placeholder": "​",
      "style": "IPY_MODEL_4ec6b684f4be4290a3777a0b2c48d0a1",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "dbfdf912175441cc80e18b47640fa3a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a8c85bb95fc454b9850349956b22fb6",
      "max": 411,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bced7d8790ad47fcabc90c0a410f310f",
      "value": 411
     }
    },
    "ae64f835eee14782af7811b4e157ca95": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61cc3bcdd4c647648948ccbe82fb8662",
      "placeholder": "​",
      "style": "IPY_MODEL_83dee3b3c4c14d4d9652fd401b275107",
      "value": " 411/411 [00:00&lt;00:00, 46.3kB/s]"
     }
    },
    "c4b51892aa5741c0bdc1ae2b2f649647": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6411565dbfb541eabaa6090e77c74a32": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ec6b684f4be4290a3777a0b2c48d0a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a8c85bb95fc454b9850349956b22fb6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bced7d8790ad47fcabc90c0a410f310f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "61cc3bcdd4c647648948ccbe82fb8662": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83dee3b3c4c14d4d9652fd401b275107": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fed7c51068f6468989ed9116a4eae0b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eaf91fdacd824007a7960f84721f0f82",
       "IPY_MODEL_e3b85304b53441a8879508e5ece97441",
       "IPY_MODEL_bc5492a7888344078b96c536bb9fa3c5"
      ],
      "layout": "IPY_MODEL_5e3ed333bc984966a71c1dd8cfec615d"
     }
    },
    "eaf91fdacd824007a7960f84721f0f82": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17fe77baeb194bf9994c3ee1a8858e4b",
      "placeholder": "​",
      "style": "IPY_MODEL_91149bad6e1e4c69ac73d1de8b472e13",
      "value": "config.json: 100%"
     }
    },
    "e3b85304b53441a8879508e5ece97441": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e82b2d91d0c54fa9aa42ce1afb67b1c3",
      "max": 594,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6fefbc5dc99241bda1f172cffb021e04",
      "value": 594
     }
    },
    "bc5492a7888344078b96c536bb9fa3c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc4f2fa028a942a2b7b31954eb41fb69",
      "placeholder": "​",
      "style": "IPY_MODEL_d07ee3e754ee4c4199b5912572bee7cd",
      "value": " 594/594 [00:00&lt;00:00, 61.9kB/s]"
     }
    },
    "5e3ed333bc984966a71c1dd8cfec615d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17fe77baeb194bf9994c3ee1a8858e4b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91149bad6e1e4c69ac73d1de8b472e13": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e82b2d91d0c54fa9aa42ce1afb67b1c3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fefbc5dc99241bda1f172cffb021e04": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dc4f2fa028a942a2b7b31954eb41fb69": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d07ee3e754ee4c4199b5912572bee7cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ac96758ad104ca190986671f0ce7204": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1657286a6dbd464b9afc692dbc6c2277",
       "IPY_MODEL_19e5f8a6d412419aa760bbe2c7703998",
       "IPY_MODEL_c6cc28f829ca4160bf29437814c24e22"
      ],
      "layout": "IPY_MODEL_b31b805e671d45cbad80e609e747f491"
     }
    },
    "1657286a6dbd464b9afc692dbc6c2277": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2038794786ae47b087755403c14dd76a",
      "placeholder": "​",
      "style": "IPY_MODEL_70250c9e1619470fbf57ed7364567a66",
      "value": "model.safetensors.index.json: 100%"
     }
    },
    "19e5f8a6d412419aa760bbe2c7703998": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91e4f44180534a1c84490ecb1b8822fe",
      "max": 26788,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c359b170ea864488b83b6c68ac2a3f3e",
      "value": 26788
     }
    },
    "c6cc28f829ca4160bf29437814c24e22": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2dfe42def69d4a538f02875bd224ae21",
      "placeholder": "​",
      "style": "IPY_MODEL_b0b2e87b7f924957b17f86c533154995",
      "value": " 26.8k/26.8k [00:00&lt;00:00, 1.65MB/s]"
     }
    },
    "b31b805e671d45cbad80e609e747f491": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2038794786ae47b087755403c14dd76a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70250c9e1619470fbf57ed7364567a66": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91e4f44180534a1c84490ecb1b8822fe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c359b170ea864488b83b6c68ac2a3f3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2dfe42def69d4a538f02875bd224ae21": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0b2e87b7f924957b17f86c533154995": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3a25ac39e874aba8d0464df2d61d48f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1cbecaed1fad4885b851cff414f7c9f1",
       "IPY_MODEL_314493a66ca44ea99d29c279dc9e0879",
       "IPY_MODEL_da91ee64f52e4dc994f7714703a6d0b1"
      ],
      "layout": "IPY_MODEL_3469279d1941476cb565ab595b44abd5"
     }
    },
    "1cbecaed1fad4885b851cff414f7c9f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08bfa39d264e4b9c9ddc48e23573239b",
      "placeholder": "​",
      "style": "IPY_MODEL_1319974df82c44da837ef03d525e7935",
      "value": "Downloading shards: 100%"
     }
    },
    "314493a66ca44ea99d29c279dc9e0879": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aba61af0e3b848339830c8af527a4925",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_358a4ee02d6243f087542c5504d9c116",
      "value": 2
     }
    },
    "da91ee64f52e4dc994f7714703a6d0b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f57b4eba8d844f2988b35edb5d410e06",
      "placeholder": "​",
      "style": "IPY_MODEL_a017dcd2eb194f489b88d95e1ce652d0",
      "value": " 2/2 [01:41&lt;00:00, 48.47s/it]"
     }
    },
    "3469279d1941476cb565ab595b44abd5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08bfa39d264e4b9c9ddc48e23573239b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1319974df82c44da837ef03d525e7935": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aba61af0e3b848339830c8af527a4925": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "358a4ee02d6243f087542c5504d9c116": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f57b4eba8d844f2988b35edb5d410e06": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a017dcd2eb194f489b88d95e1ce652d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3b7fa6a5b0a4fa686cdd60ee7cde548": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_32303f0b57ef4b3ba39cc9ebf9fe02c5",
       "IPY_MODEL_39510f633c4041dfa6d8e956dfce4b4c",
       "IPY_MODEL_244434c1d64e4b6a8034f2e964101097"
      ],
      "layout": "IPY_MODEL_7ea602a44aff45bfbfa453d02d03d349"
     }
    },
    "32303f0b57ef4b3ba39cc9ebf9fe02c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73572b79670341f8878fb2edb291ad05",
      "placeholder": "​",
      "style": "IPY_MODEL_74e1d22857ea42e9b1c224bd390223f6",
      "value": "model-00001-of-00002.safetensors: 100%"
     }
    },
    "39510f633c4041dfa6d8e956dfce4b4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f98f5cc851b4f63a158d7a2664bba2e",
      "max": 9976579217,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_99e35220e1474231995a4b37647f5e33",
      "value": 9976579217
     }
    },
    "244434c1d64e4b6a8034f2e964101097": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae75ef3b04e84cb99f3fda44b87b6693",
      "placeholder": "​",
      "style": "IPY_MODEL_fd782a58f8464963a53b18e0aee4ebc1",
      "value": " 9.98G/9.98G [01:02&lt;00:00, 220MB/s]"
     }
    },
    "7ea602a44aff45bfbfa453d02d03d349": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73572b79670341f8878fb2edb291ad05": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74e1d22857ea42e9b1c224bd390223f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f98f5cc851b4f63a158d7a2664bba2e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99e35220e1474231995a4b37647f5e33": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ae75ef3b04e84cb99f3fda44b87b6693": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd782a58f8464963a53b18e0aee4ebc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f242aac39cc4fafa21e82b6cf2e3c30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f90ba85f23304efa821786dc13a76c8d",
       "IPY_MODEL_ca1c9189e3cf4bfbaeed3e0e655e42f3",
       "IPY_MODEL_6cad3615e8924d02b28b1e3b5974f399"
      ],
      "layout": "IPY_MODEL_00e1958e09e241b6943ee217844433e3"
     }
    },
    "f90ba85f23304efa821786dc13a76c8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69ffd45bb0b04ec1ae82a58f45fa5359",
      "placeholder": "​",
      "style": "IPY_MODEL_0f8d71b0a9fb4f9286a8f07337909dbc",
      "value": "model-00002-of-00002.safetensors: 100%"
     }
    },
    "ca1c9189e3cf4bfbaeed3e0e655e42f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5965eff176364a4aaad3ab045f91b6e9",
      "max": 3500297444,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cc48ff9ea06d458694c50132878447f2",
      "value": 3500297444
     }
    },
    "6cad3615e8924d02b28b1e3b5974f399": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a4af288b269464ab9a769b52d310c9c",
      "placeholder": "​",
      "style": "IPY_MODEL_0a0ce2b818614d1198d82eb19da66d65",
      "value": " 3.50G/3.50G [00:38&lt;00:00, 132MB/s]"
     }
    },
    "00e1958e09e241b6943ee217844433e3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69ffd45bb0b04ec1ae82a58f45fa5359": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f8d71b0a9fb4f9286a8f07337909dbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5965eff176364a4aaad3ab045f91b6e9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc48ff9ea06d458694c50132878447f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a4af288b269464ab9a769b52d310c9c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a0ce2b818614d1198d82eb19da66d65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6ee030ac8b34e93aec086c62b0c2e1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_425f8a614da0420eac2a7dc9085b5436",
       "IPY_MODEL_bd48fb044fd94ba2ad000b75e5339a6d",
       "IPY_MODEL_d0a0d5ea3a124dc391c950213f56f552"
      ],
      "layout": "IPY_MODEL_b6b571d9ae5448b5b0244efb3b246ac1"
     }
    },
    "425f8a614da0420eac2a7dc9085b5436": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11d2e04398944d8abe59bd766aebff21",
      "placeholder": "​",
      "style": "IPY_MODEL_558b3735c0fd4350a068ca5206086798",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "bd48fb044fd94ba2ad000b75e5339a6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22194b7bf088407d802f882b1093f0bc",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ff3e4ff764f477b99ea0c8c2f4b889e",
      "value": 2
     }
    },
    "d0a0d5ea3a124dc391c950213f56f552": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4874b45c35a4c199df8810144c3e3e7",
      "placeholder": "​",
      "style": "IPY_MODEL_c32a697ad0df4ba39513c4e3ec60f636",
      "value": " 2/2 [00:56&lt;00:00, 25.90s/it]"
     }
    },
    "b6b571d9ae5448b5b0244efb3b246ac1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11d2e04398944d8abe59bd766aebff21": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "558b3735c0fd4350a068ca5206086798": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22194b7bf088407d802f882b1093f0bc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ff3e4ff764f477b99ea0c8c2f4b889e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c4874b45c35a4c199df8810144c3e3e7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c32a697ad0df4ba39513c4e3ec60f636": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf61365c77e5442493451eb60e0ccdab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5bc8eaf03e154cd1850d30df72261efc",
       "IPY_MODEL_853148304a5e4f7ba681814bd89e2532",
       "IPY_MODEL_49f7c3e869c149dd8b7432b38aa8d585"
      ],
      "layout": "IPY_MODEL_507ef563a12342dc9b9ef3a4e92b8a52"
     }
    },
    "5bc8eaf03e154cd1850d30df72261efc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f1a035362b145c38d4cf8b97f59d9af",
      "placeholder": "​",
      "style": "IPY_MODEL_4a3980aaec1b4449b502a122d4673bd4",
      "value": "generation_config.json: 100%"
     }
    },
    "853148304a5e4f7ba681814bd89e2532": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d16852d24914a1193173fec29dd4a9f",
      "max": 137,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c992e635d624f96a891e4930e7bc804",
      "value": 137
     }
    },
    "49f7c3e869c149dd8b7432b38aa8d585": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29c3ec6e4d8943af910c6355927b1026",
      "placeholder": "​",
      "style": "IPY_MODEL_2214b572408c4736bbb6df8d14ce677c",
      "value": " 137/137 [00:00&lt;00:00, 10.1kB/s]"
     }
    },
    "507ef563a12342dc9b9ef3a4e92b8a52": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f1a035362b145c38d4cf8b97f59d9af": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a3980aaec1b4449b502a122d4673bd4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d16852d24914a1193173fec29dd4a9f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c992e635d624f96a891e4930e7bc804": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "29c3ec6e4d8943af910c6355927b1026": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2214b572408c4736bbb6df8d14ce677c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### Practice/HW6 (10pt): Large Language Models and Their Implications\n<!-- ![img](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4470ce74-e595-4750-92a5-5f21f040df6d_577x432.jpeg) -->\n![img](https://i.imgur.com/QGYa2J8.jpeg)\n\nIn this notebook, you're gonna play with some of the largest language models on the Internet.\n\n_Based on works of: Tim Dettmers, Ruslan Svirschevsky, Artem Chumachenko, Younes Belkada, Felix Marty, Yulian Gilyazev, Gosha Zolotov, Andrey Ishutin,  Elena Volf, Artemiy Vishnyakov, Svetlana Shirokovskih. [Source notebook](https://github.com/yandexdataschool/nlp_course/tree/2024/week06_llm)",
   "metadata": {
    "id": "aSWEcS2XKgzi"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Part 1: prompt engineering (4 points total)\n\nIn the assignment, we'll use public APIs that host the 100B+ models for inference. Your task is to prompt-engineer the model into solving a few tasks for you.\n\n\n__Which API?__ You are free to use any publicly available API for general LM -- as long as it's __not a chat assistant__. So, gpt 3.5 is fine, but chatGPT is not. Here's a few options:\n\n- BLOOM API - [bigscience/bloom](https://huggingface.co/bigscience/bloom) (currently not supported)\n- OpenAI API - [openai.com/api](https://openai.com/api/)\n- Mixtral API - https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n- AI21 Jurrasic API - [ai21.com](https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1)\n\nThese APIs may require you to create a (free) account on their platform. Please note that some APIs also have paid subscriptions. __You do not need to pay them__, this assignment was designed to be solved using free-tier subscriptions. If no APIs work for you, you can also solve these tasks with the 6.7B model that you will find later in this notebook - but this will make the tasks somewhat harder.\n\n__Quests:__ you will need to solve 4 problems. For each one, please attach a short __description__ of your solution and the output from the API you use. _[If you use python APIs, show your python code with outputs]_\n\n__Example:__ Tony is talking to Darth Vader ([BLOOM API](https://huggingface.co/bigscience/bloom)). Black text is written manually, blue text is generated.\n<hr>\n\n![img](https://i.imgur.com/a1QhKF7.png)\n<hr>\n\n__It is fine to roll back a few times,__ e.g. in the example above, the model first generated Vader lines twice in a row, and we rolled that back. However, if you need more than 1-2 rollbacks per session, you should probably try a different prompt.",
   "metadata": {
    "id": "1jYrxHF8Kgzl"
   }
  },
  {
   "cell_type": "markdown",
   "source": "__Task 1 (0.5 pt):__ arange a conversation between any two of the following:\n\n- a celebrity or politician of your choice\n- any fictional character (except Darth Vader)\n- yourself\n\nCompare two setups: a) you prompt with character names only b) you supply additional information (see example).",
   "metadata": {
    "id": "CHIvIFjsKgzm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "MODEL = genai.GenerativeModel('gemini-1.5-flash')\n",
    "api_key = os.getenv(\"GOOGLEAPIKEY\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "\n",
    "# Example characters\n",
    "character1 = \"Elon Musk\"\n",
    "character2 = \"Sherlock Holmes\"\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T01:03:09.223794Z",
     "start_time": "2025-03-25T01:03:09.217456Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T01:03:28.048272Z",
     "start_time": "2025-03-25T01:03:15.136391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_A = f\"{character1} and {character2} are having a conversation. Simulate their answers\\n\\n{character1}: \"\n",
    "conversation_A = MODEL.generate_content(prompt_A).text.strip()\n",
    "print(\"### Setup A: Names Only ###\\n\", conversation_A)"
   ],
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "\n  No API_KEY or ADC found. Please either:\n    - Set the `GOOGLE_API_KEY` environment variable.\n    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\n    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mDefaultCredentialsError\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m prompt_A \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcharacter1\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcharacter2\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m are having a conversation. Simulate their answers\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mcharacter1\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m conversation_A \u001B[38;5;241m=\u001B[39m \u001B[43mMODEL\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_A\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m### Setup A: Names Only ###\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, conversation_A)\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/google/generativeai/generative_models.py:317\u001B[0m, in \u001B[0;36mGenerativeModel.generate_content\u001B[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001B[0m\n\u001B[1;32m    314\u001B[0m     request\u001B[38;5;241m.\u001B[39mcontents[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mrole \u001B[38;5;241m=\u001B[39m _USER_ROLE\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_default_generative_client\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m request_options \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    320\u001B[0m     request_options \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/google/generativeai/client.py:360\u001B[0m, in \u001B[0;36mget_default_generative_client\u001B[0;34m()\u001B[0m\n\u001B[1;32m    359\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_default_generative_client\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m glm\u001B[38;5;241m.\u001B[39mGenerativeServiceClient:\n\u001B[0;32m--> 360\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_client_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_default_client\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgenerative\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/google/generativeai/client.py:289\u001B[0m, in \u001B[0;36m_ClientManager.get_default_client\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    287\u001B[0m client \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclients\u001B[38;5;241m.\u001B[39mget(name)\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m client \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 289\u001B[0m     client \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_client\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    290\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclients[name] \u001B[38;5;241m=\u001B[39m client\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m client\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/google/generativeai/client.py:249\u001B[0m, in \u001B[0;36m_ClientManager.make_client\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ga_exceptions\u001B[38;5;241m.\u001B[39mDefaultCredentialsError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    243\u001B[0m     e\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    244\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m  No API_KEY or ADC found. Please either:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    245\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    - Set the `GOOGLE_API_KEY` environment variable.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    246\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    247\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    248\u001B[0m     )\n\u001B[0;32m--> 249\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_metadata:\n\u001B[1;32m    252\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m client\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/google/generativeai/client.py:241\u001B[0m, in \u001B[0;36m_ClientManager.make_client\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m patch_colab_gce_credentials():\n\u001B[0;32m--> 241\u001B[0m         client \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ga_exceptions\u001B[38;5;241m.\u001B[39mDefaultCredentialsError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    243\u001B[0m     e\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    244\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m  No API_KEY or ADC found. Please either:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    245\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    - Set the `GOOGLE_API_KEY` environment variable.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    246\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    247\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    248\u001B[0m     )\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:667\u001B[0m, in \u001B[0;36mGenerativeServiceClient.__init__\u001B[0;34m(self, credentials, transport, client_options, client_info)\u001B[0m\n\u001B[1;32m    658\u001B[0m     transport_init: Union[\n\u001B[1;32m    659\u001B[0m         Type[GenerativeServiceTransport],\n\u001B[1;32m    660\u001B[0m         Callable[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, GenerativeServiceTransport],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    664\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m cast(Callable[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, GenerativeServiceTransport], transport)\n\u001B[1;32m    665\u001B[0m     )\n\u001B[1;32m    666\u001B[0m     \u001B[38;5;66;03m# initialize with the provided callable or the passed in class\u001B[39;00m\n\u001B[0;32m--> 667\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transport \u001B[38;5;241m=\u001B[39m \u001B[43mtransport_init\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcredentials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcredentials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcredentials_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client_options\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcredentials_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhost\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_api_endpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscopes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client_options\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscopes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclient_cert_source_for_mtls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client_cert_source\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquota_project_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client_options\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquota_project_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclient_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclient_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43malways_use_jwt_access\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mapi_audience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client_options\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapi_audience\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masync\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transport):\n\u001B[1;32m    680\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m CLIENT_LOGGING_SUPPORTED \u001B[38;5;129;01mand\u001B[39;00m _LOGGER\u001B[38;5;241m.\u001B[39misEnabledFor(\n\u001B[1;32m    681\u001B[0m         std_logging\u001B[38;5;241m.\u001B[39mDEBUG\n\u001B[1;32m    682\u001B[0m     ):  \u001B[38;5;66;03m# pragma: NO COVER\u001B[39;00m\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:235\u001B[0m, in \u001B[0;36mGenerativeServiceGrpcTransport.__init__\u001B[0;34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001B[0m\n\u001B[1;32m    230\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ssl_channel_credentials \u001B[38;5;241m=\u001B[39m grpc\u001B[38;5;241m.\u001B[39mssl_channel_credentials(\n\u001B[1;32m    231\u001B[0m                 certificate_chain\u001B[38;5;241m=\u001B[39mcert, private_key\u001B[38;5;241m=\u001B[39mkey\n\u001B[1;32m    232\u001B[0m             )\n\u001B[1;32m    234\u001B[0m \u001B[38;5;66;03m# The base transport sets the host, credentials and scopes\u001B[39;00m\n\u001B[0;32m--> 235\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    236\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhost\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcredentials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcredentials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcredentials_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcredentials_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscopes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscopes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquota_project_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquota_project_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    241\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclient_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    242\u001B[0m \u001B[43m    \u001B[49m\u001B[43malways_use_jwt_access\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malways_use_jwt_access\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    243\u001B[0m \u001B[43m    \u001B[49m\u001B[43mapi_audience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mapi_audience\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_grpc_channel:\n\u001B[1;32m    247\u001B[0m     \u001B[38;5;66;03m# initialize with the provided callable or the default channel\u001B[39;00m\n\u001B[1;32m    248\u001B[0m     channel_init \u001B[38;5;241m=\u001B[39m channel \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreate_channel\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100\u001B[0m, in \u001B[0;36mGenerativeServiceTransport.__init__\u001B[0;34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001B[0m\n\u001B[1;32m     96\u001B[0m     credentials, _ \u001B[38;5;241m=\u001B[39m google\u001B[38;5;241m.\u001B[39mauth\u001B[38;5;241m.\u001B[39mload_credentials_from_file(\n\u001B[1;32m     97\u001B[0m         credentials_file, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mscopes_kwargs, quota_project_id\u001B[38;5;241m=\u001B[39mquota_project_id\n\u001B[1;32m     98\u001B[0m     )\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m credentials \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ignore_credentials:\n\u001B[0;32m--> 100\u001B[0m     credentials, _ \u001B[38;5;241m=\u001B[39m \u001B[43mgoogle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdefault\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mscopes_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquota_project_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquota_project_id\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;66;03m# Don't apply audience if the credentials file passed from user.\u001B[39;00m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(credentials, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwith_gdch_audience\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/google/auth/_default.py:719\u001B[0m, in \u001B[0;36mdefault\u001B[0;34m(scopes, request, quota_project_id, default_scopes)\u001B[0m\n\u001B[1;32m    711\u001B[0m             _LOGGER\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m    712\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo project ID could be determined. Consider running \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    713\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`gcloud config set project` or setting the \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    714\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menvironment variable\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    715\u001B[0m                 environment_vars\u001B[38;5;241m.\u001B[39mPROJECT,\n\u001B[1;32m    716\u001B[0m             )\n\u001B[1;32m    717\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m credentials, effective_project_id\n\u001B[0;32m--> 719\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001B[0;31mDefaultCredentialsError\u001B[0m: \n  No API_KEY or ADC found. Please either:\n    - Set the `GOOGLE_API_KEY` environment variable.\n    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\n    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "id": "yJJtTnJEKgzm",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T00:50:11.127032235Z",
     "start_time": "2025-03-24T21:44:20.778754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_B = f\"{character1} is a billionaire entrepreneur and CEO of SpaceX and Tesla. {character2} is a fictional detective known for his keen observation and deduction skills. Simulate a conversation between the two.\\n\\n {character1}: \"\n",
    "conversation_B = MODEL.generate_content(prompt_B).text.strip()\n",
    "print(\"\\n### Setup B: With extra information ###\\n\", conversation_B)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Setup B: With extra information ###\n",
      " Elon Musk:  Holmes, fascinating to finally meet you.  I've always admired your… *method*.  A purely logical approach to seemingly impossible problems.  Reminds me a bit of software debugging, actually.  Find the bug, isolate the variable, solve the equation.  Except, your \"bugs\" are usually far more… *human*.\n",
      "\n",
      "Sherlock Holmes: (Eyes Musk keenly, a slight smile playing on his lips)  Indeed, Mr. Musk.  And your \"equations,\" as you put it, involve the complexities of interstellar travel and sustainable energy.  A different scale, certainly, but the underlying principles remain the same: observation, deduction, and a healthy dose of intuition.  Though your intuition seems to involve rather more risk capital than mine.\n",
      "\n",
      "Elon Musk: (Chuckles)  Risk is inherent in innovation, Holmes.  Stagnation is the real killer.  Take Mars colonization, for instance.  The odds are long, the challenges immense… but the potential reward… well, it's rather hard to quantify.  You wouldn't understand the thrill of pushing boundaries, would you?\n",
      "\n",
      "Sherlock Holmes: (Taps a long finger against his chin)  On the contrary, Mr. Musk.  The thrill of the chase, the unraveling of a complex mystery – that is something I understand intimately.  The difference lies in the methodology.  You leap before you look, driven by a grand vision. I, however, prefer to meticulously examine each detail before forming a conclusion.  One might say your approach is…  *bold*, whereas mine is *precise*.\n",
      "\n",
      "Elon Musk:  Precise can be slow, Holmes.  Sometimes, bold is necessary to break the inertia.  We're racing against time, you know.  Climate change, resource depletion… these are problems requiring immediate, large-scale solutions.  We can't afford to meticulously examine every single variable before launching a solution.\n",
      "\n",
      "Sherlock Holmes:  (Raises an eyebrow)  Ah, but perhaps a more thorough examination of the variables might prevent a costly and ultimately ineffective \"solution.\"  I've observed that your ventures, while undeniably ambitious, have occasionally suffered from…  *oversight*.  The Falcon 9 explosions, for example.  A lack of meticulous attention to detail, I believe.\n",
      "\n",
      "Elon Musk:  Failures are learning experiences, Holmes.  Data points.  We learn from them, adapt, iterate.  It's the process of innovation.  You wouldn't understand the iterative nature of technological advancement, would you?  Building something truly groundbreaking requires taking chances.\n",
      "\n",
      "Sherlock Holmes:  (A faint amusement flickers in his eyes)  Indeed, Mr. Musk.  But even the most groundbreaking of advancements benefit from a solid foundation, constructed on the bedrock of careful observation and deduction.  Perhaps a touch more \"Holmesian\" methodology in your ventures wouldn't be entirely amiss.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": "__Please choose task 2a or 2b (0.5 pt)__ depending on your model (you can do both, but you will be awarded points for one of these two tasks).\n\n__Task 2a: (for BLOOM or other multilingual model)__ zero-shot translation. Take the first verse of [Edgar Allan Poe's \"Raven\"](https://www.poetryfoundation.org/poems/48860/the-raven) and __translate it into French.__ (You are free to use any other text of at least the same size)\n\nOriginal text: ```\nOnce upon a midnight dreary, while I pondered, weak and weary,\nOver many a quaint and curious volume of forgotten lore—\n    While I nodded, nearly napping, suddenly there came a tapping,\nAs of some one gently rapping, rapping at my chamber door.\n“’Tis some visitor,” I muttered, “tapping at my chamber door—\n            Only this and nothing more.” ```\n\nVerify your translation by converting french back into english using a public machine translation service.\n\n\n__Task 2b: (non-BLOOM)__ toxicity classification for [SetFit/toxic_conversations](https://huggingface.co/datasets/SetFit/toxic_conversations). Make the model solve binary classification (toxic vs not toxic) in the few shot mode. For few-shot examples, use 2-3 toxic and 2-3 non-toxic non-toxic examples. Measure accuracy on at least 25 samples. You may need to try several different prompts before you find the one that works.",
   "metadata": {
    "id": "Z6Bc13ueKgzn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text = '''Once upon a midnight dreary, while I pondered, weak and weary,\n",
    "Over many a quaint and curious volume of forgotten lore—\n",
    "    While I nodded, nearly napping, suddenly there came a tapping,\n",
    "As of some one gently rapping, rapping at my chamber door.\n",
    "“’Tis some visitor,” I muttered, “tapping at my chamber door—\n",
    "            Only this and nothing more.'''\n",
    "\n",
    "prompt_C = f\"{text} \\n\\nTranslate the text into French.\"\n",
    "translated_output = MODEL.generate_content(prompt_C).text.strip()\n",
    "\n",
    "print(\"\\n### Convert to french: ###\\n\", translated_output)"
   ],
   "metadata": {
    "id": "GygPXoyKKgzo",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T00:50:11.127384450Z",
     "start_time": "2025-03-24T21:48:50.913708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Convert to french: ###\n",
      " Un soir d'hiver, sombre et froid, tandis que je méditai, faible et las,\n",
      "Sur bien des volumes étranges et curieux d'un savoir oublié —\n",
      "    Alors que je somnolais, presque endormi, soudain un frappant se fit entendre,\n",
      "Comme si quelqu'un frappait doucement, frappant à ma porte de chambre.\n",
      "“C'est un visiteur,” murmurai-je, “qui frappe à ma porte de chambre —\n",
      "            Rien que cela et rien de plus.”\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "translated_output = \"\"\"Un soir d'hiver, sombre et froid, tandis que je méditai, faible et las,\n",
    "Sur bien des volumes étranges et curieux d'un savoir oublié —\n",
    "    Alors que je somnolais, presque endormi, soudain un frappant se fit entendre,\n",
    "Comme si quelqu'un frappait doucement, frappant à ma porte de chambre.\n",
    "“C'est un visiteur,” murmurai-je, “qui frappe à ma porte de chambre —\n",
    "            Rien que cela et rien de plus.\"\"\"\n",
    "\n",
    "google_translated_output = \"\"\"One winter evening, dark and cold, while I pondered, weak and weary,\n",
    "Over many strange and curious volumes of forgotten lore—\n",
    "As I dozed, almost asleep, suddenly a knocking was heard,\n",
    "As if someone were gently rapping, knocking at my chamber door.\n",
    "“It is a visitor,” I murmured, “who is knocking at my chamber door—\n",
    "Nothing but that and nothing more.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "\n__Task 3 (0.5 pt):__ create a prompt and few-shot examples tha make the model __change the gender pronouns__ of the main actor in a given sentence in any direction of your choice. E.g. the doctor took off _his_ mask <-> the doctor took of _her_ mask.\n",
   "metadata": {
    "id": "iaYweoPsKgzo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"Change the gender pronouns in these sentences. Keep all other words the same.\n",
    "\n",
    "Original: The doctor took off his mask after the surgery.\n",
    "Changed: The doctor took off her mask after the surgery.\n",
    "\n",
    "Original: She picked up her daughter from school.\n",
    "Changed: He picked up his daughter from school.\n",
    "\n",
    "Original: The nurse completed his rounds quickly.\n",
    "Changed: The nurse completed her rounds quickly.\n",
    "\n",
    "Original: The mechanic adjusted his tools before starting work.\n",
    "Changed:\"\"\"\n",
    "\n",
    "# Call the model\n",
    "response = MODEL.generate_content(prompt).text.strip()\n",
    "print(\"Response:\", response)\n",
    "\n",
    "# Test with another example\n",
    "prompt2 = prompt + \"\\n\\nOriginal: The teacher graded his papers over the weekend.\\nChanged:\"\n",
    "response2 = MODEL.generate_content(prompt2).text.strip()\n",
    "print(\"\\nResponse 2:\", response2)"
   ],
   "metadata": {
    "id": "uE-Zv_MoKgzq",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T00:50:11.128046906Z",
     "start_time": "2025-03-24T21:53:59.770495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The mechanic adjusted her tools before starting work.\n",
      "\n",
      "Response 2: Original: The mechanic adjusted his tools before starting work.\n",
      "Changed: The mechanic adjusted her tools before starting work.\n",
      "\n",
      "Original: The teacher graded his papers over the weekend.\n",
      "Changed: The teacher graded her papers over the weekend.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": "__Task 4 (0.5 pt):__ write a prompt and supply examples such that the model would __convert imperial units to metric units__ (miles -> kilometers; mph -> kph). More specifically, the model should rewrite a given sentence and replace all imperial units with their metric equivalents. After it works with basic distances and speed, try to find complicated examples where it does *not* work.\n\nPlease note that 1 mile is not equal to 1 km :)",
   "metadata": {
    "id": "bbNrRmgMKgzq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"Convert all imperial measurements to metric in these sentences. Keep all other words exactly the same.\n",
    "\n",
    "Original: The car was traveling at 60 mph on the highway.\n",
    "Converted: The car was traveling at 97 kph on the highway.\n",
    "\n",
    "Original: The hiking trail is 2.5 miles long.\n",
    "Converted: The hiking trail is 4 kilometers long.\n",
    "\n",
    "Original: My house is 1500 feet from the station.\n",
    "Converted: My house is 457 meters from the station.\n",
    "\n",
    "Original: The truck needs to deliver supplies 120 miles away.\n",
    "Converted:\"\"\"\n",
    "\n",
    "# Test basic conversion\n",
    "response = MODEL.generate_content(prompt).text.strip()\n",
    "print(\"Response:\", response)\n",
    "\n",
    "# Test with more complex example\n",
    "complex_prompt = prompt + \"\\n\\nOriginal: The car gets 35 mpg and can travel 300 miles on a full tank at 75 mph.\\nConverted:\"\n",
    "complex_response = MODEL.generate_content(complex_prompt).text.strip()\n",
    "print(\"\\nComplex Response:\", complex_response)\n",
    "\n",
    "\n",
    "hard_prompt = \"\"\"Convert all imperial measurements to metric in these sentences. Keep all other words exactly the same.\n",
    "\n",
    "Original: The engine gets 25 mpg at 65 mph with a curb weight of 4000 lbs and 18-inch wheels.\n",
    "Converted: \"\"\"\n",
    "\n",
    "response = MODEL.generate_content(hard_prompt).text.strip()\n",
    "print(\"Complex case (multiple units):\", response)\n"
   ],
   "metadata": {
    "id": "UBxMVGHNKgzr",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T00:51:19.798319Z",
     "start_time": "2025-03-25T00:51:19.778017Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 16\u001B[0m\n\u001B[1;32m      1\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mConvert all imperial measurements to metric in these sentences. Keep all other words exactly the same.\u001B[39m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;124mOriginal: The car was traveling at 60 mph on the highway.\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124mOriginal: The truck needs to deliver supplies 120 miles away.\u001B[39m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124mConverted:\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Test basic conversion\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mMODEL\u001B[49m\u001B[38;5;241m.\u001B[39mgenerate_content(prompt)\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResponse:\u001B[39m\u001B[38;5;124m\"\u001B[39m, response)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Test with more complex example\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'MODEL' is not defined"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": "### Part 2: local inference\n\nNow, let's try and load the strongest model that can fit a typical Colab GPU (T4 with 16 GB as of spring 2023).\n\nOur best candidates are the smaller versions of the best performing open source models:\n- 8 Bn parameters version of LLaMA\n- 7 Bn parameters version of [LLaMA](https://arxiv.org/pdf/2302.13971.pdf) - best for spring 2023, released by Facebook\n- 7 Bn parameters version of [Falcon](https://falconllm.tii.ae) - close competitor to Llama, released in May 2023 by [Technology Innovation Institute of UAE](https://www.tii.ae).\n- 6.7 Bn parameters version of [OPT](https://arxiv.org/abs/2205.01068) - top choice in this nomination in 2022, released by Facebook.\n\nBeware: while these models are smaller than the ones in API, they're still over 60x larger than the BERT we played with last time. The code below will *just barely* fit into memory, so make sure you don't have anything else loaded. Sometimes you may need to restart runtime for the code to work.\n\nIt's a good time to restart your kernel and switch to GPU! (Runtime -> Change runtime type)\n<center><img src=\"https://i.imgur.com/OOfDYzJ.png\" width=240px></center>",
   "metadata": {
    "id": "ZKw-mjuRKgzs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --upgrade transformers bitsandbytes accelerate sentencepiece optimum auto-gptq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from tqdm.auto import tqdm, trange\n",
    "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-03T16:38:07.582929Z",
     "iopub.execute_input": "2025-03-03T16:38:07.583197Z",
     "iopub.status.idle": "2025-03-03T16:38:31.529349Z",
     "shell.execute_reply.started": "2025-03-03T16:38:07.583170Z",
     "shell.execute_reply": "2025-03-03T16:38:31.528607Z"
    },
    "trusted": true,
    "id": "ytALQO97tnTt",
    "outputId": "9ae1b7c8-4116-42b6-e13f-65eb1a49555b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-03-25T02:59:42.532257Z",
     "start_time": "2025-03-25T02:59:35.419218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (4.50.0)\r\n",
      "Requirement already satisfied: bitsandbytes in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (0.45.3)\r\n",
      "Requirement already satisfied: accelerate in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (1.5.2)\r\n",
      "Requirement already satisfied: sentencepiece in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (0.2.0)\r\n",
      "Requirement already satisfied: optimum in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (1.24.0)\r\n",
      "Requirement already satisfied: auto-gptq in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (0.7.1)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from transformers) (0.21.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from transformers) (2.0.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: requests in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from transformers) (0.29.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: filelock in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from transformers) (3.18.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from transformers) (24.2)\r\n",
      "Requirement already satisfied: torch<3,>=2.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from bitsandbytes) (2.6.0)\r\n",
      "Requirement already satisfied: psutil in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from accelerate) (7.0.0)\r\n",
      "Requirement already satisfied: gekko in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from auto-gptq) (1.2.1)\r\n",
      "Requirement already satisfied: rouge in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from auto-gptq) (1.0.1)\r\n",
      "Requirement already satisfied: peft>=0.5.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from auto-gptq) (0.15.0)\r\n",
      "Requirement already satisfied: datasets in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from auto-gptq) (3.4.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\r\n",
      "Requirement already satisfied: jinja2 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\r\n",
      "Requirement already satisfied: networkx in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (3.2.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from datasets->auto-gptq) (19.0.1)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from datasets->auto-gptq) (0.70.16)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from datasets->auto-gptq) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from datasets->auto-gptq) (2.2.3)\r\n",
      "Requirement already satisfied: aiohttp in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from datasets->auto-gptq) (3.11.14)\r\n",
      "Requirement already satisfied: xxhash in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from datasets->auto-gptq) (3.5.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from aiohttp->datasets->auto-gptq) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from aiohttp->datasets->auto-gptq) (1.5.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from aiohttp->datasets->auto-gptq) (2.6.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from aiohttp->datasets->auto-gptq) (6.2.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from aiohttp->datasets->auto-gptq) (1.18.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from aiohttp->datasets->auto-gptq) (1.3.2)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from aiohttp->datasets->auto-gptq) (0.3.0)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from aiohttp->datasets->auto-gptq) (5.0.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from requests->transformers) (2025.1.31)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from pandas->datasets->auto-gptq) (2025.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from pandas->datasets->auto-gptq) (2025.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /users/u29/bobbyd/RIT_LLM/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets->auto-gptq) (1.17.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\r\n",
      "You should consider upgrading via the '/users/u29/bobbyd/RIT_LLM/.venv/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "If you want to use `'meta-llama/Llama-3.2-3B',` you should add the huggungface token. We use a free option",
   "metadata": {
    "id": "t8VumitqxPg6"
   }
  },
  {
   "cell_type": "code",
   "source": "# FOR SECRET ON KAGGLE KERNAL\n# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient()\n# secret_value_0 = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\n# FOR SECRET ON COLAB\n# from google.colab import userdata\n# secret_value_0 = userdata.get('HUGGINGFACE_TOKEN')\n\n#Another option:\n# from huggingface_hub import notebook_login\n# notebook_login()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-03T16:39:43.631607Z",
     "iopub.execute_input": "2025-03-03T16:39:43.631960Z",
     "iopub.status.idle": "2025-03-03T16:39:44.147510Z",
     "shell.execute_reply.started": "2025-03-03T16:39:43.631935Z",
     "shell.execute_reply": "2025-03-03T16:39:44.146586Z"
    },
    "trusted": true,
    "id": "nN4ULulYtnTu"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Below is an example of a simple pipeline, that you don't want in general to use. That's why it's commented",
   "metadata": {
    "id": "OsKlOczGwrNB"
   }
  },
  {
   "cell_type": "code",
   "source": "# from transformers import pipeline\n# model_id = 'huggyllama/llama-7b' # \"meta-llama/Llama-3.2-3B\"\n\n# pipe = pipeline(\n#     \"text-generation\",\n#     model=model_id,\n#     torch_dtype=torch.bfloat16,\n#     device_map=\"auto\",\n#     token=secret_value_0\n# )\n\n# pipe(\"The key to life is\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-03T16:39:54.813014Z",
     "iopub.execute_input": "2025-03-03T16:39:54.813331Z",
     "iopub.status.idle": "2025-03-03T16:39:54.816812Z",
     "shell.execute_reply.started": "2025-03-03T16:39:54.813306Z",
     "shell.execute_reply": "2025-03-03T16:39:54.815932Z"
    },
    "trusted": true,
    "id": "0QrmWnrKtnTu",
    "outputId": "33423324-ba4f-4dce-b0b0-bd4ef2a79f2f",
    "colab": {
     "referenced_widgets": [
      "a97b5f09901b497ebf3c7f96d7938372"
     ]
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Text generation\n\n**Comparison of strategies for language model text generation:**\n\n| Strategy | Description | Pros & Cons |\n| --- | --- | --- |\n| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br> **Cons:** Can lead to repetitive and incoherent text. |\n| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br> **Cons:** Setting an optimal 'p' can be tricky. |\n| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br> **Cons:** Can lack diversity and lead to generic responses. |\n| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br> **Cons:** Tuning the normalization factor can be difficult. |\n| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br> **Cons:** Computationally more complex and requires a good loss function. |\n\nDocumentation references:\n- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)",
   "metadata": {
    "id": "5k2zCgAhG7l5"
   }
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-03T16:39:59.479985Z",
     "iopub.execute_input": "2025-03-03T16:39:59.480388Z",
     "iopub.status.idle": "2025-03-03T16:40:03.291829Z",
     "shell.execute_reply.started": "2025-03-03T16:39:59.480355Z",
     "shell.execute_reply": "2025-03-03T16:40:03.291111Z"
    },
    "trusted": true,
    "id": "IZvFk4uWtnTv",
    "ExecuteTime": {
     "end_time": "2025-03-25T02:59:17.441716Z",
     "start_time": "2025-03-25T02:59:17.438636Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "model_name = 'huggyllama/llama-7b' # 'meta-llama/Llama-3.2-3B'\ntokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device)\n# tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device, token=secret_value_0)\ntokenizer.pad_token_id = tokenizer.eos_token_id\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map='auto',\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    offload_state_dict=True,\n    # token=secret_value_0\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-03T16:40:21.081971Z",
     "iopub.execute_input": "2025-03-03T16:40:21.082418Z",
     "iopub.status.idle": "2025-03-03T16:40:21.086210Z",
     "shell.execute_reply.started": "2025-03-03T16:40:21.082393Z",
     "shell.execute_reply": "2025-03-03T16:40:21.085216Z"
    },
    "trusted": true,
    "id": "kbXnaNvqtnTv",
    "outputId": "316a0793-3d22-4c16-96c4-997bfd9a91b0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514,
     "referenced_widgets": [
      "4ac6df2f421f4dcfac60ffd52407a843",
      "023eb3bc3896450dbe5aa7c7751beb7a",
      "45e880576e1243ff922c2e51e33f9cb5",
      "dbb9b51b0d5e49b2aecf571091162f80",
      "b1497a51af1f472e93471b79a430618b",
      "582d5b88d04e4fd5b4bc8b06dfde6e90",
      "d60635aaa32b463a933a96b36aabe3a6",
      "1f542a31fc8847a3bf408ff1de8af112",
      "e2462efd37374e4a8520dfad584bb970",
      "f370af5a686046e99fea5246e7b40317",
      "43015ad798d8483690c854411ebdfa80",
      "e1226e5154dd4c8591310fbaa8b0a9fd",
      "b4d564dcb7de4c498fbe842aae305f0d",
      "34e0050072754f128271fc0819b30328",
      "eb2a0adbeccd4536ae316d3b37757dd4",
      "378a372042fe4d3696b08a647b4967d2",
      "30c9cd3caf4d4b4bb79e65744ba3601a",
      "0b05bf0430904f148fa55b3118533e57",
      "dd6aab2782624c1c8220512f7324b016",
      "1bcf5a22fe9242168a364b1a6b2fa337",
      "2ecf28296e0346a4af6da53aa73b8579",
      "aa558b3efff64710ad70f581d62bd269",
      "e07aac9228914b499ae718db3f9ba789",
      "d5446e9a0ff24fcf868ad67d37e53f68",
      "38b3f9f2df97419c872fb3ff878735ce",
      "e1c34849111f4be084c83eea3230d553",
      "e0f07bf1fb5f44dea503b48c64c8b81c",
      "29f0f81e20a24fb4aee94a1256b8f6b6",
      "e19a002be9d44e8988e268b82414739b",
      "4b21d61300c8437d84ff3a2184067278",
      "9284a4cad25249248a93b13087c9592c",
      "324c045b882b4eab929cad294083c6da",
      "d7f42e43ec7449278b0ff59b483064a3",
      "304da90c6f18498981684233abc064b0",
      "5da053d61e0e41b0a85d5503f54cc25a",
      "dbfdf912175441cc80e18b47640fa3a3",
      "ae64f835eee14782af7811b4e157ca95",
      "c4b51892aa5741c0bdc1ae2b2f649647",
      "6411565dbfb541eabaa6090e77c74a32",
      "4ec6b684f4be4290a3777a0b2c48d0a1",
      "6a8c85bb95fc454b9850349956b22fb6",
      "bced7d8790ad47fcabc90c0a410f310f",
      "61cc3bcdd4c647648948ccbe82fb8662",
      "83dee3b3c4c14d4d9652fd401b275107",
      "fed7c51068f6468989ed9116a4eae0b5",
      "eaf91fdacd824007a7960f84721f0f82",
      "e3b85304b53441a8879508e5ece97441",
      "bc5492a7888344078b96c536bb9fa3c5",
      "5e3ed333bc984966a71c1dd8cfec615d",
      "17fe77baeb194bf9994c3ee1a8858e4b",
      "91149bad6e1e4c69ac73d1de8b472e13",
      "e82b2d91d0c54fa9aa42ce1afb67b1c3",
      "6fefbc5dc99241bda1f172cffb021e04",
      "dc4f2fa028a942a2b7b31954eb41fb69",
      "d07ee3e754ee4c4199b5912572bee7cd",
      "9ac96758ad104ca190986671f0ce7204",
      "1657286a6dbd464b9afc692dbc6c2277",
      "19e5f8a6d412419aa760bbe2c7703998",
      "c6cc28f829ca4160bf29437814c24e22",
      "b31b805e671d45cbad80e609e747f491",
      "2038794786ae47b087755403c14dd76a",
      "70250c9e1619470fbf57ed7364567a66",
      "91e4f44180534a1c84490ecb1b8822fe",
      "c359b170ea864488b83b6c68ac2a3f3e",
      "2dfe42def69d4a538f02875bd224ae21",
      "b0b2e87b7f924957b17f86c533154995",
      "c3a25ac39e874aba8d0464df2d61d48f",
      "1cbecaed1fad4885b851cff414f7c9f1",
      "314493a66ca44ea99d29c279dc9e0879",
      "da91ee64f52e4dc994f7714703a6d0b1",
      "3469279d1941476cb565ab595b44abd5",
      "08bfa39d264e4b9c9ddc48e23573239b",
      "1319974df82c44da837ef03d525e7935",
      "aba61af0e3b848339830c8af527a4925",
      "358a4ee02d6243f087542c5504d9c116",
      "f57b4eba8d844f2988b35edb5d410e06",
      "a017dcd2eb194f489b88d95e1ce652d0",
      "d3b7fa6a5b0a4fa686cdd60ee7cde548",
      "32303f0b57ef4b3ba39cc9ebf9fe02c5",
      "39510f633c4041dfa6d8e956dfce4b4c",
      "244434c1d64e4b6a8034f2e964101097",
      "7ea602a44aff45bfbfa453d02d03d349",
      "73572b79670341f8878fb2edb291ad05",
      "74e1d22857ea42e9b1c224bd390223f6",
      "6f98f5cc851b4f63a158d7a2664bba2e",
      "99e35220e1474231995a4b37647f5e33",
      "ae75ef3b04e84cb99f3fda44b87b6693",
      "fd782a58f8464963a53b18e0aee4ebc1",
      "4f242aac39cc4fafa21e82b6cf2e3c30",
      "f90ba85f23304efa821786dc13a76c8d",
      "ca1c9189e3cf4bfbaeed3e0e655e42f3",
      "6cad3615e8924d02b28b1e3b5974f399",
      "00e1958e09e241b6943ee217844433e3",
      "69ffd45bb0b04ec1ae82a58f45fa5359",
      "0f8d71b0a9fb4f9286a8f07337909dbc",
      "5965eff176364a4aaad3ab045f91b6e9",
      "cc48ff9ea06d458694c50132878447f2",
      "6a4af288b269464ab9a769b52d310c9c",
      "0a0ce2b818614d1198d82eb19da66d65",
      "c6ee030ac8b34e93aec086c62b0c2e1c",
      "425f8a614da0420eac2a7dc9085b5436",
      "bd48fb044fd94ba2ad000b75e5339a6d",
      "d0a0d5ea3a124dc391c950213f56f552",
      "b6b571d9ae5448b5b0244efb3b246ac1",
      "11d2e04398944d8abe59bd766aebff21",
      "558b3735c0fd4350a068ca5206086798",
      "22194b7bf088407d802f882b1093f0bc",
      "1ff3e4ff764f477b99ea0c8c2f4b889e",
      "c4874b45c35a4c199df8810144c3e3e7",
      "c32a697ad0df4ba39513c4e3ec60f636",
      "bf61365c77e5442493451eb60e0ccdab",
      "5bc8eaf03e154cd1850d30df72261efc",
      "853148304a5e4f7ba681814bd89e2532",
      "49f7c3e869c149dd8b7432b38aa8d585",
      "507ef563a12342dc9b9ef3a4e92b8a52",
      "4f1a035362b145c38d4cf8b97f59d9af",
      "4a3980aaec1b4449b502a122d4673bd4",
      "1d16852d24914a1193173fec29dd4a9f",
      "8c992e635d624f96a891e4930e7bc804",
      "29c3ec6e4d8943af910c6355927b1026",
      "2214b572408c4736bbb6df8d14ce677c"
     ]
    },
    "ExecuteTime": {
     "end_time": "2025-03-25T03:03:19.589821Z",
     "start_time": "2025-03-25T03:03:08.808772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.95s/it]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "JEr93yGHw7gN"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "You can use different generation strategies",
   "metadata": {
    "id": "AsIcOQqVzJHq"
   }
  },
  {
   "cell_type": "code",
   "source": "from transformers import GenerationConfig",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-03T16:40:56.091627Z",
     "iopub.execute_input": "2025-03-03T16:40:56.091927Z",
     "iopub.status.idle": "2025-03-03T16:40:56.095460Z",
     "shell.execute_reply.started": "2025-03-03T16:40:56.091906Z",
     "shell.execute_reply": "2025-03-03T16:40:56.094601Z"
    },
    "trusted": true,
    "id": "nTNcniGdtnTv",
    "ExecuteTime": {
     "end_time": "2025-03-25T03:03:22.902329Z",
     "start_time": "2025-03-25T03:03:22.899058Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "# Define generation config\ngeneration_config = GenerationConfig(\n    max_length=200,\n    temperature=0.8,\n    top_p=0.9,\n    do_sample=True\n)\n\n# greedy inference:                                        do_sample=False)\n# beam search for highest probability:                     num_beams=4)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-03T16:41:01.394195Z",
     "iopub.execute_input": "2025-03-03T16:41:01.394523Z",
     "iopub.status.idle": "2025-03-03T16:41:01.398320Z",
     "shell.execute_reply.started": "2025-03-03T16:41:01.394476Z",
     "shell.execute_reply": "2025-03-03T16:41:01.397397Z"
    },
    "trusted": true,
    "id": "sCojahP4tnTv",
    "ExecuteTime": {
     "end_time": "2025-03-25T03:03:24.143903Z",
     "start_time": "2025-03-25T03:03:24.140595Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "prompt = 'The first discovered martian lifeform looks like' #'The key to life is'\nbatch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\nprint(\"Input batch (encoded):\", batch)\n\noutput_tokens = model.generate(**batch, generation_config=generation_config)\n\nprint(\"\\nOutput:\", tokenizer.decode(output_tokens[0].cpu()))",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T00:58:29.996175Z",
     "iopub.status.busy": "2025-02-27T00:58:29.995850Z",
     "iopub.status.idle": "2025-02-27T00:58:39.202353Z",
     "shell.execute_reply": "2025-02-27T00:58:39.201632Z",
     "shell.execute_reply.started": "2025-02-27T00:58:29.996153Z"
    },
    "trusted": true,
    "id": "6X47qwK_tnTw",
    "outputId": "f3ee5bcd-b479-48f9-d7b4-ee1fa1503179",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-03-25T03:03:39.582785Z",
     "start_time": "2025-03-25T03:03:26.716055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch (encoded): {'input_ids': tensor([[    1,   450,   937, 10943, 14436,   713,  2834,   689,  3430,   763]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "\n",
      "Output: <s> The first discovered martian lifeform looks like an organic jellyfish that can survive in Martian surface.\n",
      "The first discovered martian lifeform looks like an organic jellyfish that can survive in Martian surface. The jellyfish is called Colonists of Mars, is a microorganism discovered by the NASA.\n",
      "The researchers found that the life form of the jellyfish is very much like the organic microbes that exist on earth. But the jellyfish is very resilient to the very harsh conditions in Mars.\n",
      "\"The jellyfish is very resilient to the very harsh conditions on Mars,\" said one of the researchers, Jill Mikucki.\n",
      "Mikucki and her colleagues were conducting research to determine how microorganisms are surviving in the Red Planet's soil.\n",
      "They analyzed samples of soil and found the presence of microbes. However, when\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": "### You can use quntized model's weights (the following code does the quantization on the fly)",
   "metadata": {
    "id": "Tkbp6YMaz7UF"
   }
  },
  {
   "cell_type": "code",
   "source": "from transformers import BitsAndBytesConfig",
   "metadata": {
    "id": "h6MYvsy_0JDg",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-03T16:41:07.961384Z",
     "iopub.execute_input": "2025-03-03T16:41:07.961716Z",
     "iopub.status.idle": "2025-03-03T16:41:07.972376Z",
     "shell.execute_reply.started": "2025-03-03T16:41:07.961692Z",
     "shell.execute_reply": "2025-03-03T16:41:07.971461Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-25T03:04:09.025763Z",
     "start_time": "2025-03-25T03:04:09.022524Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n# quantization_config = BitsAndBytesConfig(\n#     load_in_4bit=True, #or load_in_8bit=True\n#     bnb_4bit_use_double_quant=True,\n#     bnb_4bit_quant_type=\"nf4\"\n#     )",
   "metadata": {
    "id": "XwkuQWTs0Skn",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-03T16:41:16.247466Z",
     "iopub.execute_input": "2025-03-03T16:41:16.247811Z",
     "iopub.status.idle": "2025-03-03T16:41:16.251366Z",
     "shell.execute_reply.started": "2025-03-03T16:41:16.247786Z",
     "shell.execute_reply": "2025-03-03T16:41:16.250593Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-25T03:04:10.394241Z",
     "start_time": "2025-03-25T03:04:10.391338Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "model_name = 'huggyllama/llama-7b' # 'meta-llama/Llama-3.2-3B'\ntokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device)\n# tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device, token=secret_value_0)\ntokenizer.pad_token_id = tokenizer.eos_token_id\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=quantization_config,  #old way: load_in_8bit=True\n    device_map='auto',\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    offload_state_dict=True,\n    # token=secret_value_0\n    )",
   "metadata": {
    "id": "OjdBzI1R0OkP",
    "outputId": "b64835da-cc8e-4ce0-b8b6-26c2d4068f00",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-03T17:15:33.601179Z",
     "iopub.execute_input": "2025-03-03T17:15:33.601532Z",
     "iopub.status.idle": "2025-03-03T17:15:33.605430Z",
     "shell.execute_reply.started": "2025-03-03T17:15:33.601475Z",
     "shell.execute_reply": "2025-03-03T17:15:33.604604Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-25T03:04:26.295174Z",
     "start_time": "2025-03-25T03:04:12.830947Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.17s/it]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": "### Or you can upload a quantized model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model_name = \"TheBloke/Llama-2-13B-GPTQ\" #meta-llama/Llama-3-8B-GPTQ\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n#tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n#model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=False, revision=\"main\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-03T17:10:06.039565Z",
     "iopub.execute_input": "2025-03-03T17:10:06.040000Z",
     "iopub.status.idle": "2025-03-03T17:10:06.043376Z",
     "shell.execute_reply.started": "2025-03-03T17:10:06.039951Z",
     "shell.execute_reply": "2025-03-03T17:10:06.042601Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-25T00:50:11.132362827Z",
     "start_time": "2025-03-24T23:27:37.807242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23b63bdfa9e24940a19b426b74f734da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e00405958f6e4bb0864d8aba96fa5fef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b83a2cbb6944402b43515ed95ea34eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9f208e6faf148cab1e7619dd584d09c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/913 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56031709780a48dc924fec78a69656f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/u29/bobbyd/Documents/Week02_classification/.venv/lib64/python3.9/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
      "/users/u29/bobbyd/Documents/Week02_classification/.venv/lib64/python3.9/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/users/u29/bobbyd/Documents/Week02_classification/.venv/lib64/python3.9/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/7.26G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4be90636a3f843dca0f68a3fdeb07925"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/users/u29/bobbyd/Documents/Week02_classification/.venv/lib64/python3.9/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 164970624 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at TheBloke/Llama-2-13B-GPTQ were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.32.mlp.down_proj.bias', 'model.layers.32.mlp.gate_proj.bias', 'model.layers.32.mlp.up_proj.bias', 'model.layers.32.self_attn.k_proj.bias', 'model.layers.32.self_attn.o_proj.bias', 'model.layers.32.self_attn.q_proj.bias', 'model.layers.32.self_attn.v_proj.bias', 'model.layers.33.mlp.down_proj.bias', 'model.layers.33.mlp.gate_proj.bias', 'model.layers.33.mlp.up_proj.bias', 'model.layers.33.self_attn.k_proj.bias', 'model.layers.33.self_attn.o_proj.bias', 'model.layers.33.self_attn.q_proj.bias', 'model.layers.33.self_attn.v_proj.bias', 'model.layers.34.mlp.down_proj.bias', 'model.layers.34.mlp.gate_proj.bias', 'model.layers.34.mlp.up_proj.bias', 'model.layers.34.self_attn.k_proj.bias', 'model.layers.34.self_attn.o_proj.bias', 'model.layers.34.self_attn.q_proj.bias', 'model.layers.34.self_attn.v_proj.bias', 'model.layers.35.mlp.down_proj.bias', 'model.layers.35.mlp.gate_proj.bias', 'model.layers.35.mlp.up_proj.bias', 'model.layers.35.self_attn.k_proj.bias', 'model.layers.35.self_attn.o_proj.bias', 'model.layers.35.self_attn.q_proj.bias', 'model.layers.35.self_attn.v_proj.bias', 'model.layers.36.mlp.down_proj.bias', 'model.layers.36.mlp.gate_proj.bias', 'model.layers.36.mlp.up_proj.bias', 'model.layers.36.self_attn.k_proj.bias', 'model.layers.36.self_attn.o_proj.bias', 'model.layers.36.self_attn.q_proj.bias', 'model.layers.36.self_attn.v_proj.bias', 'model.layers.37.mlp.down_proj.bias', 'model.layers.37.mlp.gate_proj.bias', 'model.layers.37.mlp.up_proj.bias', 'model.layers.37.self_attn.k_proj.bias', 'model.layers.37.self_attn.o_proj.bias', 'model.layers.37.self_attn.q_proj.bias', 'model.layers.37.self_attn.v_proj.bias', 'model.layers.38.mlp.down_proj.bias', 'model.layers.38.mlp.gate_proj.bias', 'model.layers.38.mlp.up_proj.bias', 'model.layers.38.self_attn.k_proj.bias', 'model.layers.38.self_attn.o_proj.bias', 'model.layers.38.self_attn.q_proj.bias', 'model.layers.38.self_attn.v_proj.bias', 'model.layers.39.mlp.down_proj.bias', 'model.layers.39.mlp.gate_proj.bias', 'model.layers.39.mlp.up_proj.bias', 'model.layers.39.self_attn.k_proj.bias', 'model.layers.39.self_attn.o_proj.bias', 'model.layers.39.self_attn.q_proj.bias', 'model.layers.39.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72272df8e5e2442ea98b1317ff8c9145"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "prompt = 'The first discovered martian lifeform looks like' #'The key to life is'\nbatch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\nprint(\"Input batch (encoded):\", batch)\n\noutput_tokens = model.generate(**batch, generation_config=generation_config)\n\nprint(\"\\nOutput:\", tokenizer.decode(output_tokens[0].cpu()))\n#print(tokenizer.decode(output[0], skip_special_tokens=True))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-03T16:59:12.015251Z",
     "iopub.execute_input": "2025-03-03T16:59:12.015972Z",
     "iopub.status.idle": "2025-03-03T17:04:21.834033Z",
     "shell.execute_reply.started": "2025-03-03T16:59:12.015938Z",
     "shell.execute_reply": "2025-03-03T17:04:21.833240Z"
    },
    "trusted": true,
    "id": "Mws9fsY4tnTw",
    "outputId": "c9327536-dc06-4b2b-e501-b862fede46ba",
    "ExecuteTime": {
     "end_time": "2025-03-25T00:50:11.132778852Z",
     "start_time": "2025-03-25T00:49:06.337741Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe first discovered martian lifeform looks like\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;66;03m#'The key to life is'\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m(prompt, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m, return_token_type_ids\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput batch (encoded):\u001B[39m\u001B[38;5;124m\"\u001B[39m, batch)\n\u001B[1;32m      5\u001B[0m output_tokens \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbatch, generation_config\u001B[38;5;241m=\u001B[39mgeneration_config)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "#### Low-level code for text generation",
   "metadata": {
    "id": "P17ehC1sKgzx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# You may need to clear GPU memory\n",
    "# import gc\n",
    "# del output_tokens\n",
    "# del model\n",
    "# del tokenizer\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-03T17:12:20.101763Z",
     "iopub.execute_input": "2025-03-03T17:12:20.102058Z",
     "iopub.status.idle": "2025-03-03T17:12:21.287797Z",
     "shell.execute_reply.started": "2025-03-03T17:12:20.102037Z",
     "shell.execute_reply": "2025-03-03T17:12:21.286913Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-25T01:07:49.684875Z",
     "start_time": "2025-03-25T01:07:49.229678Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "model_name = 'huggyllama/llama-7b' # 'meta-llama/Llama-3.2-3B'\ntokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device)\ntokenizer.pad_token_id = tokenizer.eos_token_id\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map='auto',\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    offload_state_dict=True\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-03T17:12:38.008944Z",
     "iopub.execute_input": "2025-03-03T17:12:38.009234Z",
     "iopub.status.idle": "2025-03-03T17:13:25.786351Z",
     "shell.execute_reply.started": "2025-03-03T17:12:38.009213Z",
     "shell.execute_reply": "2025-03-03T17:13:25.785500Z"
    },
    "trusted": true,
    "id": "AmnmqNXStnTw",
    "ExecuteTime": {
     "end_time": "2025-03-25T03:05:00.362180Z",
     "start_time": "2025-03-25T03:04:26.546922Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.00s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "prompt = \"New York is the capital of\"\n# prompt = \"Skippy, a young android, likes to dream about electric\"\n\nprint(prompt, '\\n')\n\nvoc = tokenizer.get_vocab()\nvoc_rev = {v:k for k, v in voc.items()}  # reverse vocab for decode\n\nfor i in range(10):\n    inputs = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n    logits = model.forward(**inputs).logits[0, -1, :]\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    next_token_id = torch.multinomial(probs.flatten(), num_samples=1)\n\n    next_token = tokenizer.decode(next_token_id)\n    prompt += next_token\n\n    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n    top_tokens = sorted_indices[:5]\n    print(f\"Step #{i} candidates:\")\n    for t, p in zip (top_tokens, sorted_probs):\n        t = voc_rev[t.item()]\n        print(f\"{t:<10}: {p:.4f} \")\n\n    print(f'\\nChosen token: {next_token}', end='\\n\\n', flush=True)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-03-03T17:13:31.214713Z",
     "iopub.execute_input": "2025-03-03T17:13:31.214995Z",
     "iopub.status.idle": "2025-03-03T17:13:32.841448Z",
     "shell.execute_reply.started": "2025-03-03T17:13:31.214975Z",
     "shell.execute_reply": "2025-03-03T17:13:32.840768Z"
    },
    "id": "LZJvOMbmG7l8",
    "outputId": "1fe924b7-3b66-41b9-99c0-16619b064035",
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:08:12.470925Z",
     "start_time": "2025-03-25T03:07:47.648942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York is the capital of \n",
      "\n",
      "Step #0 candidates:\n",
      "▁the      : 0.6230 \n",
      "▁fashion  : 0.0363 \n",
      "▁America  : 0.0320 \n",
      "▁New      : 0.0196 \n",
      "▁American : 0.0114 \n",
      "\n",
      "Chosen token: the\n",
      "\n",
      "Step #1 candidates:\n",
      "▁United   : 0.3350 \n",
      "▁world    : 0.1375 \n",
      "▁USA      : 0.0923 \n",
      "▁state    : 0.0696 \n",
      "▁fashion  : 0.0446 \n",
      "\n",
      "Chosen token: American\n",
      "\n",
      "Step #2 candidates:\n",
      "▁state    : 0.1855 \n",
      "▁Empire   : 0.0994 \n",
      "▁economy  : 0.0544 \n",
      "▁financial: 0.0338 \n",
      "▁film     : 0.0261 \n",
      "\n",
      "Chosen token: film\n",
      "\n",
      "Step #3 candidates:\n",
      "▁industry : 0.3816 \n",
      "indust    : 0.2107 \n",
      ".         : 0.0663 \n",
      "▁and      : 0.0481 \n",
      ",         : 0.0435 \n",
      "\n",
      "Chosen token: industry\n",
      "\n",
      "Step #4 candidates:\n",
      ".         : 0.5132 \n",
      ",         : 0.2277 \n",
      "and       : 0.0662 \n",
      "▁and      : 0.0492 \n",
      "<0x0A>    : 0.0135 \n",
      "\n",
      "Chosen token: .\n",
      "\n",
      "Step #5 candidates:\n",
      "<0x0A>    : 0.1089 \n",
      "▁It       : 0.0854 \n",
      "▁The      : 0.0569 \n",
      "It        : 0.0372 \n",
      "The       : 0.0351 \n",
      "\n",
      "Chosen token: My\n",
      "\n",
      "Step #6 candidates:\n",
      "f         : 0.0793 \n",
      "▁favorite : 0.0392 \n",
      "first     : 0.0349 \n",
      "▁first    : 0.0268 \n",
      "▁favour   : 0.0224 \n",
      "\n",
      "Chosen token: friend\n",
      "\n",
      "Step #7 candidates:\n",
      ",         : 0.1052 \n",
      "and       : 0.0399 \n",
      "▁and      : 0.0390 \n",
      "st        : 0.0363 \n",
      "sh        : 0.0243 \n",
      "\n",
      "Chosen token: of\n",
      "\n",
      "Step #8 candidates:\n",
      "many      : 0.1077 \n",
      "mine      : 0.1052 \n",
      "th        : 0.0472 \n",
      "the       : 0.0243 \n",
      "▁         : 0.0205 \n",
      "\n",
      "Chosen token: th\n",
      "\n",
      "Step #9 candidates:\n",
      "em        : 0.2632 \n",
      "irty      : 0.1808 \n",
      "et        : 0.1442 \n",
      "el        : 0.0655 \n",
      "ed        : 0.0312 \n",
      "\n",
      "Chosen token: irty\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": "### Part 3: Chain-of-thought prompting (4 points total)\n\n![img](https://github.com/kojima-takeshi188/zero_shot_cot/raw/main/img/image_stepbystep.png)\n\n---\n\n",
   "metadata": {
    "id": "5ZaQZhPXOPSG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random\n",
    "import locale; locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "# !wget https://raw.githubusercontent.com/kojima-takeshi188/zero_shot_cot/2824685e25809779dbd36900a69825068e9f51ef/dataset/AQuA/test.json -O aqua.json\n",
    "data = list(map(json.loads, open(\"aqua.json\")))"
   ],
   "metadata": {
    "id": "N2AmfelTn5en",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T02:59:02.428520Z",
     "start_time": "2025-03-25T02:59:02.421932Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "print(\"Example:\")\ndata[150]",
   "metadata": {
    "id": "IATXmPfYw8s6",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T02:59:02.561469Z",
     "start_time": "2025-03-25T02:59:02.438575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?',\n",
       " 'options': ['A)1 minute',\n",
       "  'B)2 minutes',\n",
       "  'C)3 minutes',\n",
       "  'D)4 minutes',\n",
       "  'E)5 minutes'],\n",
       " 'rationale': \"Janice's speed = 1/6 miles per minute\\nJennie's speed = 1/3 miles per minute\\nJanice + Jennie's speed= (1/6 + 1/3) = 1/2 miles per minute\\nBoth together will finish the mile in 2 minutes\\ncorrect option is B\",\n",
       " 'correct': 'B'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "### Naive solution\n\nHere, we prompt the model to choose an answer to the example above (`data[150]`) out of the options given above. We're using a format that mimics grade school solution textbook.\n\nPlease note that there are minor formatting changes in options: an extra space and an opening bracket. Those may or may not be important :)",
   "metadata": {
    "id": "6UcOYQPW8sVq"
   }
  },
  {
   "cell_type": "code",
   "source": "EXAMPLE_0SHOT = \"\"\"\nQuestion: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\nAnswer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\nCorrect Answer:\n\"\"\".strip()",
   "metadata": {
    "id": "KtkkdiJl3-UI",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T02:59:02.590645Z",
     "start_time": "2025-03-25T02:59:02.565479Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "# solving an equation directly\nbatch = tokenizer(EXAMPLE_0SHOT, return_tensors='pt', return_token_type_ids=False).to(device)\ntorch.manual_seed(1337)\noutput_tokens = model.generate(**batch, max_new_tokens=100, do_sample=True, top_p=0.9)\nprint(\"[Prompt:]\\n\" + EXAMPLE_0SHOT)\nprint(\"=\" * 80)\nprint(\"[Generated:]\", tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu()))",
   "metadata": {
    "id": "hyQ_8tJc6nyv",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:11:32.213046Z",
     "start_time": "2025-03-25T03:08:12.476193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prompt:]\n",
      "Question: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\n",
      "Answer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\n",
      "Correct Answer:\n",
      "================================================================================\n",
      "[Generated:] 5 minutes.\n",
      "Given, Janice bikes at 10 miles per hour.\n",
      "Therefore, to travel 1 mile, she has to travel 10 minutes\n",
      "Similarly, Jennie bikes at 20 miles per hour.\n",
      "Therefore, to travel 1 mile, she has to travel 20 minutes.\n",
      "Therefore, the time taken by Janice and Jennie to collectively travel 1 mile is 5 minutes\n",
      "The\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": "And here's how you can solve this with few-shot chain-of-thought prompting.\n\nYou need to chang 3 things\n- use a new field called **Rationale**, that contains a step-by-step solution to the problem\n- add several few-shot examples of previously solved problems **with rationales**\n- change the final prompt so that the model has to generate rationale before answering",
   "metadata": {
    "id": "suSkiDk28I6C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "EXAMPLE_3SHOT_CHAIN_OF_THOUGHT = \"\"\"\n",
    "Question: The original retail price of an appliance was 60 percent more than its wholesale cost. If the appliance was actually sold for 20 percent less than the original retail price, then it was sold for what percent more than its wholesale cost?\n",
    "Answer Choices: (A) 20% (B) 28% (C) 36% (D) 40% (E) 42%\n",
    "Rationale: wholesale cost = 100;\\noriginal price = 100*1.6 = 160;\\nactual price = 160*0.8 = 128.\\nAnswer: B.\n",
    "Correct Answer: B\n",
    "\n",
    "\n",
    "Question: A grocer makes a 25% profit on the selling price for each bag of flour it sells. If he sells each bag for $100 and makes $3,000 in profit, how many bags did he sell?\n",
    "Answer Choices: (A) 12 (B) 16 (C) 24 (D) 30 (E) 40\n",
    "Rationale: Profit on one bag: 100*1.25= 125\\nNumber of bags sold = 3000/125 = 24\\nAnswer is C.\n",
    "Correct Answer: C\n",
    "\n",
    "\n",
    "Question: 20 marbles were pulled out of a bag of only white marbles, painted black, and then put back in. Then, another 20 marbles were pulled out, of which 1 was black, after which they were all returned to the bag. If the percentage of black marbles pulled out the second time represents their percentage in the bag, how many marbles in total Q does the bag currently hold?\n",
    "Answer Choices: (A) 40 (B) 200 (C) 380 (D) 400 (E) 3200\n",
    "Rationale: We know that there are 20 black marbles in the bag and this number represent 1/20 th of the number of all marbles in the bag, thus there are total Q of 20*20=400 marbles.\\nAnswer: D.\n",
    "Correct Answer: D\n",
    "\n",
    "\n",
    "Question: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\n",
    "Answer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\n",
    "Rationale:\n",
    "\"\"\".strip()"
   ],
   "metadata": {
    "id": "K0F1jYdRvoJW",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:12:03.310620Z",
     "start_time": "2025-03-25T03:12:03.306742Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": "batch = tokenizer(EXAMPLE_3SHOT_CHAIN_OF_THOUGHT, return_tensors='pt', return_token_type_ids=False).to(device)\ntorch.manual_seed(1337)\noutput_tokens = model.generate(**batch, max_new_tokens=100, do_sample=True, top_p=0.9)\nprint(\"[Prompt:]\\n\" + EXAMPLE_3SHOT_CHAIN_OF_THOUGHT)\nprint(\"=\" * 80)\nprint(\"[Generated:]\", tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu()))\n#### NOTE: scroll down for the final answer (below the ======= line)",
   "metadata": {
    "id": "Tn8QoAYcRkHC",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T02:59:02.750060113Z",
     "start_time": "2025-03-25T01:08:59.940935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prompt:]\n",
      "Question: The original retail price of an appliance was 60 percent more than its wholesale cost. If the appliance was actually sold for 20 percent less than the original retail price, then it was sold for what percent more than its wholesale cost?\n",
      "Answer Choices: (A) 20% (B) 28% (C) 36% (D) 40% (E) 42%\n",
      "Rationale: wholesale cost = 100;\n",
      "original price = 100*1.6 = 160;\n",
      "actual price = 160*0.8 = 128.\n",
      "Answer: B.\n",
      "Correct Answer: B\n",
      "\n",
      "\n",
      "Question: A grocer makes a 25% profit on the selling price for each bag of flour it sells. If he sells each bag for $100 and makes $3,000 in profit, how many bags did he sell?\n",
      "Answer Choices: (A) 12 (B) 16 (C) 24 (D) 30 (E) 40\n",
      "Rationale: Profit on one bag: 100*1.25= 125\n",
      "Number of bags sold = 3000/125 = 24\n",
      "Answer is C.\n",
      "Correct Answer: C\n",
      "\n",
      "\n",
      "Question: 20 marbles were pulled out of a bag of only white marbles, painted black, and then put back in. Then, another 20 marbles were pulled out, of which 1 was black, after which they were all returned to the bag. If the percentage of black marbles pulled out the second time represents their percentage in the bag, how many marbles in total Q does the bag currently hold?\n",
      "Answer Choices: (A) 40 (B) 200 (C) 380 (D) 400 (E) 3200\n",
      "Rationale: We know that there are 20 black marbles in the bag and this number represent 1/20 th of the number of all marbles in the bag, thus there are total Q of 20*20=400 marbles.\n",
      "Answer: D.\n",
      "Correct Answer: D\n",
      "\n",
      "\n",
      "Question: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\n",
      "Answer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\n",
      "Rationale:\n",
      "================================================================================\n",
      "[Generated:] 10 + 20 = 30 miles per hour and since the time required for a mile is 6 minutes, we get the answer 10/6 = 1/3 minutes\n",
      "Answer: A.\n",
      "Correct Answer: A\n",
      "\n",
      "\n",
      "Question: If 3/5 of a number is 4, what is the number?\n",
      "Answer Choices: (A) 1 (B) 3/2 (C) 4/3 (\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": "__Task 5 (3 pt)__ write a function that automatically creates chain-of-thought prompts. Follow the instructions from the function docstring.",
   "metadata": {
    "id": "s4px3jv-99-m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "QUESTION_PREFIX = \"Question: \"\n",
    "OPTIONS_PREFIX = \"Answer Choices: \"\n",
    "CHAIN_OF_THOUGHT_PREFIX = \"Rationale: \"\n",
    "ANSWER_PREFIX = \"Correct Answer: \"\n",
    "FEWSHOT_SEPARATOR = \"\\n\\n\\n\"\n",
    "\n",
    "def format_example(example):\n",
    "    question = example['question'].strip()\n",
    "    options = \" \".join([f\"({opt.split(')')[0]}) {opt.split(')')[1]}\" for opt in example['options']])\n",
    "    rationale = example[\"rationale\"]\n",
    "    answer = example['correct']\n",
    "    return f\"{QUESTION_PREFIX}{question}\\n{OPTIONS_PREFIX}{options}\\n{CHAIN_OF_THOUGHT_PREFIX}{rationale}\\n{ANSWER_PREFIX}{answer}\"\n",
    "\n",
    "def make_prompt(*, main_question, fewshot_examples=None):\n",
    "    if fewshot_examples is None:\n",
    "        return f\"{QUESTION_PREFIX}{main_question['question'].strip()}\\n{OPTIONS_PREFIX}\" + \\\n",
    "                              \" \".join([f\"({opt.split(')')[0]}) {opt.split(')')[1]}\"  for opt in main_question['options']]) + \"\\n\" + CHAIN_OF_THOUGHT_PREFIX\n",
    "    formatted_fewshot_examples = [format_example(example) for example in fewshot_examples]\n",
    "    formatted_main_question = f\"{QUESTION_PREFIX}{main_question['question'].strip()}\\n{OPTIONS_PREFIX}\" + \\\n",
    "                              \" \".join([f\"({opt.split(')')[0]}) {opt.split(')')[1]}\"  for opt in main_question['options']])\n",
    "    prompt = FEWSHOT_SEPARATOR.join(formatted_fewshot_examples) + FEWSHOT_SEPARATOR + formatted_main_question + \"\\n\" + CHAIN_OF_THOUGHT_PREFIX\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "\n",
    "generated_fewshot_prompt = make_prompt(main_question=data[150], fewshot_examples=(data[30], data[20], data[5]))\n",
    "\n",
    "\n",
    "assert generated_fewshot_prompt == EXAMPLE_3SHOT_CHAIN_OF_THOUGHT, \"prompts don't match\"\n",
    "assert generated_fewshot_prompt != make_prompt(main_question=data[150], fewshot_examples=())\n",
    "assert generated_fewshot_prompt.endswith(make_prompt(main_question=data[150], fewshot_examples=()))\n",
    "\n",
    "print(\"Well done!\")\n",
    "\n",
    "# Hint: if two prompts do not match, you may find it usefull to use https://www.diffchecker.com or similar to find the difference"
   ],
   "metadata": {
    "id": "_ntyFPMt9fyt",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:12:11.406122Z",
     "start_time": "2025-03-25T03:12:11.397950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": "__Task 6 (3 points):__ Evaluate your prompt.\n\nPlease run the model on the entire dataset and measure it's accuracy.\nFor each question, peak $n=5$ other questions at random to serve as few-shot examples. Make sure not to accidentally sample the main_question among few-shot examples. For scientific evaluation, it is also a good practice to split the data into two parts: one for eval, and another for few-shot examples. However, doing so is optional in this homework.\n\nThe tricky part is when to stop generating: if you don't control for this, your model can accidentally generate a whole new question - and promptyly answer it :) To make sure you get the correct answer, stop generating tokens when the model is done explaining it's solution. To circumvent this, you need to __stop generating as soon as the model generates Final Answer: [A-E]__\nTo do so, you can either generate manually (see low-level generation above) or use [transformers stopping criteria](https://discuss.huggingface.co/t/implimentation-of-stopping-criteria-list/20040/2), whichever you prefer.\n\nIf you do everything right, the model should be much better than random. However, please __do not expect miracles__: this is far from the best models, and it will perform much worse than an average human.",
   "metadata": {
    "id": "P7DzQ8hfOcFR"
   }
  },
  {
   "cell_type": "code",
   "source": "NUM_SAMPLES = 0    # use this to count how many samples you evaluated\nNUM_RESPONDED = 0  # how many times did the model produce Correct Answer: (letter) in it's response. use as a sanity check.\nNUM_CORRECT = 0    # how many times did the model's chosen answer (letter) match the correct answer",
   "metadata": {
    "id": "oMvj9eCtQwnz",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:12:13.993157Z",
     "start_time": "2025-03-25T03:12:13.990164Z"
    }
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import gc\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Define stopping criteria\n",
    "class StopOnFinalAnswer(StoppingCriteria):\n",
    "    def __init__(self, stop_string):\n",
    "        self.stop_string = stop_string\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        decoded_output = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return self.stop_string in decoded_output\n",
    "\n",
    "eval_data = data[:100]  # Replace with your evaluation data\n",
    "fewshot_data = data[100:]  # Replace with your few-shot data\n",
    "\n",
    "BATCH_SIZE = 10  # Define the batch size\n",
    "\n",
    "# Function to create batches\n",
    "def create_batches(data, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "for batch in create_batches(eval_data, BATCH_SIZE):\n",
    "    prompts = []\n",
    "    for main_question in batch:\n",
    "        fewshot_examples = random.sample(fewshot_data, 5)\n",
    "        prompt = make_prompt(main_question=main_question, fewshot_examples=fewshot_examples)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnFinalAnswer(ANSWER_PREFIX)])\n",
    "\n",
    "    outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    for generated_text, main_question in zip(generated_texts, batch):\n",
    "        NUM_SAMPLES += 1\n",
    "        last_line = generated_text.split(\"\\n\")[-1]\n",
    "        if ANSWER_PREFIX in last_line:\n",
    "            final_answer = last_line.split(ANSWER_PREFIX)[-1].strip()\n",
    "\n",
    "            if final_answer in \"ABCDE\":\n",
    "                NUM_RESPONDED += 1\n",
    "                if final_answer == main_question['correct']:\n",
    "                    NUM_CORRECT += 1\n",
    "\n",
    "\n",
    "\n",
    "# Optionally, consider inferencing multiple sentences in a batch for faster inference;\n",
    "# If you choose to batch outputs, make sure the results are the same as with batch=1 (using greedy inference)"
   ],
   "metadata": {
    "id": "Zh6gejr0JNuh",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:42:46.040306Z",
     "start_time": "2025-03-25T03:42:40.116861Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 264.00 MiB. GPU 0 has a total capacity of 47.43 GiB of which 193.75 MiB is free. Process 4050693 has 21.12 GiB memory in use. Including non-PyTorch memory, this process has 26.11 GiB memory in use. Of the allocated memory 24.37 GiB is allocated by PyTorch, and 1.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 39\u001B[0m\n\u001B[1;32m     36\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m tokenizer(prompts, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     37\u001B[0m stopping_criteria \u001B[38;5;241m=\u001B[39m StoppingCriteriaList([StopOnFinalAnswer(ANSWER_PREFIX)])\n\u001B[0;32m---> 39\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m generated_texts \u001B[38;5;241m=\u001B[39m [tokenizer\u001B[38;5;241m.\u001B[39mdecode(output, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs]\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m generated_text, main_question \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(generated_texts, batch):\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/transformers/generation/utils.py:2326\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001B[0m\n\u001B[1;32m   2318\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   2319\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   2320\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[1;32m   2321\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   2322\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   2323\u001B[0m     )\n\u001B[1;32m   2325\u001B[0m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[0;32m-> 2326\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2327\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2328\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2329\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2330\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2331\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2332\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2333\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2334\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2336\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[1;32m   2337\u001B[0m     \u001B[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001B[39;00m\n\u001B[1;32m   2338\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   2339\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   2340\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[1;32m   2341\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   2342\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   2343\u001B[0m     )\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/transformers/generation/utils.py:3286\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[1;32m   3283\u001B[0m model_inputs\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_hidden_states\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[1;32m   3285\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_prefill:\n\u001B[0;32m-> 3286\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   3287\u001B[0m     is_prefill \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   3288\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/accelerate/hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/transformers/utils/deprecation.py:172\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action\u001B[38;5;241m.\u001B[39mNOTIFY, Action\u001B[38;5;241m.\u001B[39mNOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/transformers/models/llama/modeling_llama.py:853\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m    850\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m    852\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m--> 853\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    856\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    857\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    858\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    859\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    860\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    861\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    863\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    867\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    868\u001B[0m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/transformers/models/llama/modeling_llama.py:601\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001B[0m\n\u001B[1;32m    589\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    590\u001B[0m         decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    591\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    598\u001B[0m         position_embeddings,\n\u001B[1;32m    599\u001B[0m     )\n\u001B[1;32m    600\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 601\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    602\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    604\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mflash_attn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    611\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    613\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    615\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/accelerate/hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/transformers/models/llama/modeling_llama.py:359\u001B[0m, in \u001B[0;36mLlamaDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    357\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    358\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_attention_layernorm(hidden_states)\n\u001B[0;32m--> 359\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    360\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    362\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (hidden_states,)\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/accelerate/hooks.py:176\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/RIT_LLM/.venv/lib64/python3.9/site-packages/transformers/models/llama/modeling_llama.py:197\u001B[0m, in \u001B[0;36mLlamaMLP.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 197\u001B[0m     down_proj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdown_proj(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgate_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mup_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m down_proj\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 264.00 MiB. GPU 0 has a total capacity of 47.43 GiB of which 193.75 MiB is free. Process 4050693 has 21.12 GiB memory in use. Including non-PyTorch memory, this process has 26.11 GiB memory in use. Of the allocated memory 24.37 GiB is allocated by PyTorch, and 1.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": "print(\"Responded %%:\", NUM_RESPONDED / NUM_SAMPLES)\nprint(\"Accuracy (when responded):\", NUM_CORRECT / NUM_RESPONDED)\nprint(\"Accuracy (overall):\", NUM_CORRECT / NUM_SAMPLES)\n\nif NUM_RESPONDED / NUM_SAMPLES < 0.9:\n  print(\"Something is wrong with the evaluation technique (for 5-shot CoT): the model refuses to answer too many questions.\")\n  print(\"Make sure you generate enough tokens that the model can produce a correct answer.\")\n  print(\"When in doubt, take a look at the full model output. You can often spot errors there.\")",
   "metadata": {
    "id": "gxrCJvxIJSjA",
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T03:42:46.042797093Z",
     "start_time": "2025-03-25T03:16:02.393651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responded %%: 1.0\n",
      "Accuracy (when responded): 0.29\n",
      "Accuracy (overall): 0.29\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": "__Task 7 (2 points)__ Experiment time!\n<img width=200px src=https://www.evolvefish.com/cdn-cgi/image/quality%3D85/assets/images/Apparel/TShirtsWomenCont/Main/EF-APP-CWT-00068(Main).jpg>\n\nYour final quest is to use the testbench you've just written to answer one of the following questions:\n\n### Option 1: How many shots do you need?\n\nHow does model accuracy change with the number of fewshot examples?\n\na. check if the model accuracy changes as you increase/decrease the number of \"shots\"\n\nb. try to prompt-engineer a model into giving the best rationale __without__ any few-shot examples, i.e. zero-shot\n\nFor zero-shot mode, feel free to use wild prompt-engineering or modify the inference procedure.\n\n### Option 2: Is this prompting tecnique reliable?\n\n_Inspired by ongoing research by Anton Voronov, Lena Volf and Max Ryabinin._\n\nFor this option, you need to check if the model behavior (and hence, accuracy) is robust to perturbations in the input prompt.\n\na. Does the accuracy degrade if you provide wrong answers to few-shot examples? (make sure to modify rationale if it contains answer in the end)\n\nb. Does it degrade if you replace question/answer prompts with \"Q\" and \"A\"? What if you write both on the same line? Change few-shot separators?\n\n\n\n### Option 3: Inference Matters\n\nThere are many ways to inference the model, not all of them equal.\n\na. check whether greedy inference or beam search affects model generation quality\n\nb. implement and evaluate sampling with voting (see explanation below).\n\n\nThe voting technique(b) should work as follows: first, you generate k (e.g. 50) \"attempts\" at an answer using nucleus sampling (or a similar technique).\nThen, you count how many of those attempts chose a particular option (A, B, etc) as the final answer. The option that was chosen most frequently has the most \"votes\", and therefore \"wins\".\n\nTo speed up voting, you may want to generate these attempts in parallel as a batch. That should be very easy to implement: just run `model.generate` on a list with multiple copies of the same prompt.\n\n\n\n\n================================================\n\n__Common rules:__ You will need to test both hypothes (A and B) in the chosen option. You may choose to replace one of them with your own idea - but please ask course staff in advance (via telegram) if you want full points.\n\nFeel free to organize your code and report as you see fit - but please make sure it's readable and the code runs top-to-bottom :)\nWrite a short informal report about what you tried and, in doing so, what did you found. Minimum of 2 paragraphs; more is ok; creative visualizations are welcome.\n\nYou are allowed (but not required) to prompt the model into generating a report for you --- or helping you write one. However, if you do so, make sure that it is still human-readable :)\n\n",
   "metadata": {
    "id": "UZLK2rLiKxbM"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1 shot"
  },
  {
   "cell_type": "code",
   "source": [
    "# feel free to organize your solution as you see it\n",
    "import random\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "NUM_SAMPLES = 0    # use this to count how many samples you evaluated\n",
    "NUM_RESPONDED = 0  # how many times did the model produce Correct Answer: (letter) in it's response. use as a sanity check.\n",
    "NUM_CORRECT = 0\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Define stopping criteria\n",
    "class StopOnFinalAnswer(StoppingCriteria):\n",
    "    def __init__(self, stop_string):\n",
    "        self.stop_string = stop_string\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        decoded_output = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return self.stop_string in decoded_output\n",
    "\n",
    "eval_data = data[:100]  # Replace with your evaluation data\n",
    "fewshot_data = data[100:]  # Replace with your few-shot data\n",
    "\n",
    "BATCH_SIZE = 10  # Define the batch size\n",
    "\n",
    "# Function to create batches\n",
    "def create_batches(data, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "for batch in create_batches(eval_data, BATCH_SIZE):\n",
    "    prompts = []\n",
    "    for main_question in batch:\n",
    "        fewshot_examples = random.sample(fewshot_data, 1)\n",
    "        prompt = make_prompt(main_question=main_question, fewshot_examples=fewshot_examples)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnFinalAnswer(ANSWER_PREFIX)])\n",
    "\n",
    "    outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    for generated_text, main_question in zip(generated_texts, batch):\n",
    "        NUM_SAMPLES += 1\n",
    "        last_line = generated_text.split(\"\\n\")[-1]\n",
    "        if ANSWER_PREFIX in last_line:\n",
    "            final_answer = last_line.split(ANSWER_PREFIX)[-1].strip()\n",
    "\n",
    "            if final_answer in \"ABCDE\":\n",
    "                NUM_RESPONDED += 1\n",
    "                if final_answer == main_question['correct']:\n",
    "                    NUM_CORRECT += 1\n",
    "\n",
    "\n",
    "\n",
    "# Optionally, consider inferencing multiple sentences in a batch for faster inference;\n",
    "# If you choose to batch outputs, make sure the results are the same as with batch=1 (using greedy inference)\n",
    "print(\"Results with 1 shot:\")\n",
    "print(\"Responded %%:\", NUM_RESPONDED / NUM_SAMPLES)\n",
    "print(\"Accuracy (when responded):\", NUM_CORRECT / NUM_RESPONDED)\n",
    "print(\"Accuracy (overall):\", NUM_CORRECT / NUM_SAMPLES)\n"
   ],
   "metadata": {
    "id": "_r6UVDl4NEua",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10 Shot"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# feel free to organize your solution as you see it\n",
    "import random\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "NUM_SAMPLES = 0    # use this to count how many samples you evaluated\n",
    "NUM_RESPONDED = 0  # how many times did the model produce Correct Answer: (letter) in it's response. use as a sanity check.\n",
    "NUM_CORRECT = 0\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Define stopping criteria\n",
    "class StopOnFinalAnswer(StoppingCriteria):\n",
    "    def __init__(self, stop_string):\n",
    "        self.stop_string = stop_string\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        decoded_output = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return self.stop_string in decoded_output\n",
    "\n",
    "eval_data = data[:100]  # Replace with your evaluation data\n",
    "fewshot_data = data[100:]  # Replace with your few-shot data\n",
    "\n",
    "BATCH_SIZE = 10  # Define the batch size\n",
    "\n",
    "# Function to create batches\n",
    "def create_batches(data, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "for batch in create_batches(eval_data, BATCH_SIZE):\n",
    "    prompts = []\n",
    "    for main_question in batch:\n",
    "        fewshot_examples = random.sample(fewshot_data, 3)\n",
    "        prompt = make_prompt(main_question=main_question, fewshot_examples=fewshot_examples)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnFinalAnswer(ANSWER_PREFIX)])\n",
    "\n",
    "    outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    for generated_text, main_question in zip(generated_texts, batch):\n",
    "        NUM_SAMPLES += 1\n",
    "        last_line = generated_text.split(\"\\n\")[-1]\n",
    "        if ANSWER_PREFIX in last_line:\n",
    "            final_answer = last_line.split(ANSWER_PREFIX)[-1].strip()\n",
    "\n",
    "            if final_answer in \"ABCDE\":\n",
    "                NUM_RESPONDED += 1\n",
    "                if final_answer == main_question['correct']:\n",
    "                    NUM_CORRECT += 1\n",
    "\n",
    "\n",
    "# Optionally, consider inferencing multiple sentences in a batch for faster inference;\n",
    "# If you choose to batch outputs, make sure the results are the same as with batch=1 (using greedy inference)\n",
    "print(\"Results with 10 shot:\")\n",
    "print(\"Responded %%:\", NUM_RESPONDED / NUM_SAMPLES)\n",
    "print(\"Accuracy (when responded):\", NUM_CORRECT / NUM_RESPONDED)\n",
    "print(\"Accuracy (overall):\", NUM_CORRECT / NUM_SAMPLES)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## New Prompt with zero shot"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# feel free to organize your solution as you see it\n",
    "import random\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "NUM_SAMPLES = 0    # use this to count how many samples you evaluated\n",
    "NUM_RESPONDED = 0  # how many times did the model produce Correct Answer: (letter) in it's response. use as a sanity check.\n",
    "NUM_CORRECT = 0\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Define stopping criteria\n",
    "class StopOnFinalAnswer(StoppingCriteria):\n",
    "    def __init__(self, stop_string):\n",
    "        self.stop_string = stop_string\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        decoded_output = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return self.stop_string in decoded_output\n",
    "\n",
    "eval_data = data[:100]  # Replace with your evaluation data\n",
    "fewshot_data = data[100:]  # Replace with your few-shot data\n",
    "\n",
    "BATCH_SIZE = 10  # Define the batch size\n",
    "\n",
    "# Function to create batches\n",
    "def create_batches(data, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "for batch in create_batches(eval_data, BATCH_SIZE):\n",
    "    prompts = []\n",
    "    for main_question in batch:\n",
    "        fewshot_examples = random.sample(fewshot_data, 0)\n",
    "\n",
    "        prompt = f\"{QUESTION_PREFIX}{main_question['question'].strip()}\\n{OPTIONS_PREFIX}\" + \\\n",
    "                              \" \".join([f\"({opt.split(')')[0]}) {opt.split(')')[1]}\"  for opt in main_question['options']]) + \"Include your step by step thinking on a new line preceded with 'Rationale:' \\n\" + CHAIN_OF_THOUGHT_PREFIX\n",
    "\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnFinalAnswer(ANSWER_PREFIX)])\n",
    "\n",
    "    outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    for generated_text, main_question in zip(generated_texts, batch):\n",
    "        NUM_SAMPLES += 1\n",
    "        last_line = generated_text.split(\"\\n\")[-1]\n",
    "        if ANSWER_PREFIX in last_line:\n",
    "            final_answer = last_line.split(ANSWER_PREFIX)[-1].strip()\n",
    "\n",
    "            if final_answer in \"ABCDE\":\n",
    "                NUM_RESPONDED += 1\n",
    "                if final_answer == main_question['correct']:\n",
    "                    NUM_CORRECT += 1\n",
    "\n",
    "\n",
    "# Optionally, consider inferencing multiple sentences in a batch for faster inference;\n",
    "# If you choose to batch outputs, make sure the results are the same as with batch=1 (using greedy inference)\n",
    "print(\"Results with 0 shot and engineering:\")\n",
    "print(\"Responded %%:\", NUM_RESPONDED / NUM_SAMPLES)\n",
    "print(\"Accuracy (when responded):\", NUM_CORRECT / NUM_RESPONDED)\n",
    "print(\"Accuracy (overall):\", NUM_CORRECT / NUM_SAMPLES)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Report\n",
    "\n",
    "First, the few-shot stuff. As expected, my accuracy definitely climbed as I increased the number of examples. For simpler tasks, like basic arithmetic or straightforward classification, I saw a pretty steep jump in accuracy with just a few examples – like, going from guessing to near-perfect with 3-5 shots. However, for more complex tasks, like multi-step reasoning or nuanced language understanding, the gains were slower and more gradual. I also noticed that the quality of the examples was crucial.\n",
    "\n",
    "I feel like after a certain number of examples, adding more wouldn't improve accuracy much, due to the model absorbing all the relevant patterns. Zero shot prompting did not offer much benefit with prompt engineering. In my experience the accuracy only improved as a result of the multi shot. If the shots were increased and the prompt was engineered then I feel like that would result in the best results."
   ]
  }
 ]
}
