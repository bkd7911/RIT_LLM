{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwmTTyjUGqol"
   },
   "source": [
    "### Build a GPT transformer\n",
    "\n",
    "In this section, you will implement a transformer language model layer by layer, then use it to generate coherent (hopefully) text.\n",
    "\n",
    "To understand how these layers work, you can check the guide to transformers from [nlp course for you -> transformers](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#transformer_intro).\n",
    "\n",
    "\n",
    "First, we download pre-trained weights for the [GPT2 model by OpenAI](https://openai.com/research/better-language-models) -- a prominent model from 2019.\n",
    "\n",
    "\n",
    "\n",
    "Idea & code by: Ilya Beletsky\n",
    "\n",
    "We used the `seminar.ipynb` from the [source](https://github.com/yandexdataschool/nlp_course/tree/2024/week05_transfer)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zriTdjauH8iQ",
    "ExecuteTime": {
     "end_time": "2025-03-24T04:59:39.135808Z",
     "start_time": "2025-03-24T04:59:37.752518Z"
    }
   },
   "source": [
    "# %pip install huggingface_hub\n",
    "import transformers\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vOcK0lGTGqol",
    "ExecuteTime": {
     "end_time": "2025-03-24T05:00:10.855558Z",
     "start_time": "2025-03-24T05:00:10.498002Z"
    }
   },
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "state_dict = torch.load(hf_hub_download(\"gpt2\", filename=\"pytorch_model.bin\"))\n",
    "for key, value in tuple(state_dict.items()):\n",
    "    if key.startswith('h.') and key.endswith('.weight') and value.ndim == 2:\n",
    "        value.transpose_(1, 0)  # <-- for compatibility with modern PyTorch modules\n",
    "    if key.startswith('h.') and key.endswith('.attn.bias') and value.ndim == 4:\n",
    "        state_dict.pop(key)  # <-- triangular binar masks, not needed in this code\n",
    "\n",
    "print('Weights:', repr(sorted(state_dict.keys()))[:320], '...')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: ['h.0.attn.c_attn.bias', 'h.0.attn.c_attn.weight', 'h.0.attn.c_proj.bias', 'h.0.attn.c_proj.weight', 'h.0.ln_1.bias', 'h.0.ln_1.weight', 'h.0.ln_2.bias', 'h.0.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.0.mlp.c_fc.weight', 'h.0.mlp.c_proj.bias', 'h.0.mlp.c_proj.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.c_attn.weight', 'h.1. ...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mr0SUtQnGqom"
   },
   "source": [
    "In the next few cells, we shall implement the model layer by layer to make use of those weights.\n",
    "\n",
    "As you might recall, transformers contain two main layer types: attention and fully-connected layers.\n",
    "\n",
    "The fully connected layers are by far easier to understand, so we shall begin there:\n",
    "\n",
    "Please implement fully-connected layer __without residual or layer normalization__ (we'll add those in a bit)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3Rh-6DX9Gqom",
    "ExecuteTime": {
     "end_time": "2025-03-24T05:00:15.241924Z",
     "start_time": "2025-03-24T05:00:15.239283Z"
    }
   },
   "source": [
    "class GeLUThatWasUsedInGPT2(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * x ** 3)))\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(dim, 4  * dim)\n",
    "        self.gelu = GeLUThatWasUsedInGPT2()\n",
    "        self.c_proj = nn.Linear(4 * dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = [batch_size, seq_length, dim]\n",
    "        x = self.c_fc(x)  # Apply the first linear layer\n",
    "        x = self.gelu(x)  # Apply the GeLU activation function\n",
    "        x = self.c_proj(x)  # Apply the second linear layer\n",
    "        return x  # Return the output of the MLP\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSVGKnHBGqom"
   },
   "source": [
    "Now, let's test that it works with GPT-2 weights:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CoWjZwZkGqom",
    "ExecuteTime": {
     "end_time": "2025-03-24T05:00:17.124194Z",
     "start_time": "2025-03-24T05:00:17.044796Z"
    }
   },
   "source": [
    "mlp = FullyConnected(dim=768)\n",
    "mlp.load_state_dict({'c_fc.weight': state_dict['h.0.mlp.c_fc.weight'],\n",
    "                     'c_fc.bias': state_dict['h.0.mlp.c_fc.bias'],\n",
    "                     'c_proj.weight': state_dict['h.0.mlp.c_proj.weight'],\n",
    "                     'c_proj.bias': state_dict['h.0.mlp.c_proj.bias']})\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "x = torch.randn(1, 2, 768)  # [batch_size, sequence_length, dim]\n",
    "checksum = torch.sum(mlp(x) * x)\n",
    "assert abs(checksum.item() - 1282.3315) < 0.1, \"layer outputs do not match reference\"\n",
    "assert torch.allclose(mlp(x[:, (1, 0), :])[:, (1, 0), :], mlp(x)), \"mlp must be permutation-invariant\"\n",
    "print(\"Seems legit!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems legit!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbfCevRwGqom"
   },
   "source": [
    "Now, let's get to attention layers.\n",
    "\n",
    "Since GPT-2 needs to generate text from left to right, each generated token can only attend to tokens on the left (and itself). This kid of attention is called \"Masked\" self-attention, because it hides tokens to the right.\n",
    "\n",
    "As before, please implement masked self-attention __without layernorm or residual connections.__"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T6j7M4hLGqon",
    "ExecuteTime": {
     "end_time": "2025-03-24T05:02:23.785714Z",
     "start_time": "2025-03-24T05:02:23.778171Z"
    }
   },
   "source": [
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(dim, dim * 3)  # query + key + value, combined\n",
    "        self.c_proj = nn.Linear(dim, dim)  # output projection\n",
    "        self.dim, self.num_heads = dim, num_heads\n",
    "        self.head_size = dim // num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "\n",
    "        # Split into query, key, value projections\n",
    "        q, k, v = self.c_attn(x).split(self.dim, dim=-1)\n",
    "        assert q.shape == k.shape == v.shape == x.shape, \"q, k and v must have the same shape as x\"\n",
    "\n",
    "        # Reshape query, key, value for multi-head attention\n",
    "        q = q.view(batch_size, seq_length, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_length, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_length, self.num_heads, self.head_size).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        scale = 1.0 / math.sqrt(self.head_size)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "\n",
    "        # Create causal mask\n",
    "        causal_mask = torch.triu(torch.ones(seq_length, seq_length, dtype=torch.bool), diagonal=1)\n",
    "        scores.masked_fill_(causal_mask.to(scores.device), float('-inf'))\n",
    "\n",
    "        # Apply softmax and compute weighted sum\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # Reshape and project output\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, seq_length, self.dim)\n",
    "\n",
    "        return self.c_proj(attn_output)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test that it works"
   ],
   "metadata": {
    "id": "umZpcpIkJva7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "attn = MaskedSelfAttention(dim=768, num_heads=12)\n",
    "attn.load_state_dict({'c_attn.weight': state_dict['h.0.attn.c_attn.weight'],\n",
    "                      'c_attn.bias': state_dict['h.0.attn.c_attn.bias'],\n",
    "                      'c_proj.weight': state_dict['h.0.attn.c_proj.weight'],\n",
    "                      'c_proj.bias': state_dict['h.0.attn.c_proj.bias']})\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "x = torch.randn(1, 10, 768)  # [batch_size, sequence_length, dim]\n",
    "checksum = torch.sum(attn(x) * x)\n",
    "assert abs(checksum.item() - 2703.6772) < 0.1, \"layer outputs do not match reference\"\n",
    "assert not torch.allclose(attn(x[:, (1, 0), :])[:, (1, 0), :], attn(x[:, (0, 1), :])), \"masked attention must *not* be permutation-invariant\"\n",
    "print(\"It works!\")"
   ],
   "metadata": {
    "id": "tg5Oj_PPM6hj",
    "ExecuteTime": {
     "end_time": "2025-03-24T05:02:33.561368Z",
     "start_time": "2025-03-24T05:02:33.431116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It works!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now combine attention and MLP to build the full transformer layer:\n",
    "\n",
    "![img](https://i.imgur.com/1sq2vHO.png)"
   ],
   "metadata": {
    "id": "rn6tgTHzOK4l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.attn = MaskedSelfAttention(dim, num_heads)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        self.mlp = FullyConnected(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First sublayer: Self-attention with residual connection\n",
    "        attn_output = self.attn(self.ln_1(x))\n",
    "        x = x + attn_output\n",
    "\n",
    "        # Second sublayer: MLP with residual connection\n",
    "        mlp_output = self.mlp(self.ln_2(x))\n",
    "        x = x + mlp_output\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "id": "p3AH7YQvRpvU",
    "ExecuteTime": {
     "end_time": "2025-03-24T05:03:43.171909Z",
     "start_time": "2025-03-24T05:03:43.158913Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "layer = TransformerLayer(dim=768, num_heads=12)\n",
    "layer.load_state_dict({k[5:]: v for k, v in state_dict.items() if k.startswith('h.10.')})\n",
    "assert abs(torch.sum(layer(x) * x).item() - 9874.7383) < 0.1\n",
    "print(\"Good job!\")"
   ],
   "metadata": {
    "id": "Qzo_QeFVSNZa",
    "ExecuteTime": {
     "end_time": "2025-03-24T05:03:44.363384Z",
     "start_time": "2025-03-24T05:03:44.306382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job!\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, vocab_size: int, dim: int, num_heads: int, num_layers: int, max_position_embeddings: int = 1024):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, dim)  # token embeddings\n",
    "        self.wpe = nn.Embedding(max_position_embeddings, dim)  # position embeddings\n",
    "        self.ln_f = nn.LayerNorm(dim)   # final layer norm - goes after all transformer layers, but before logits\n",
    "\n",
    "        self.h = nn.Sequential(*(TransformerLayer(dim, num_heads) for layer in range(num_layers)))\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids.shape: [batch_size, sequence_length], int64 token ids\n",
    "        position_ids = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        token_embeddings = self.wte(input_ids)\n",
    "        position_embeddings = self.wpe(position_ids)\n",
    "        full_embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "        transformer_output = self.h(full_embeddings)\n",
    "        transformer_output_ln = self.ln_f(transformer_output)\n",
    "\n",
    "        # final layer: we predict logits by re-using token embeddings as linear weights\n",
    "        output_logits = transformer_output_ln @ self.wte.weight.T\n",
    "        return output_logits\n"
   ],
   "metadata": {
    "id": "Mbqw9iuaSrYy",
    "ExecuteTime": {
     "end_time": "2025-03-24T05:03:48.083201Z",
     "start_time": "2025-03-24T05:03:48.079858Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
    "model = GPT2(vocab_size=50257, dim=768, num_heads=12, num_layers=12)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "input_ids = tokenizer(\"A quick\", return_tensors='pt')['input_ids']\n",
    "\n",
    "predicted_logits = model(input_ids)\n",
    "most_likely_token_id = predicted_logits[:, -1].argmax().item()\n",
    "\n",
    "print(\"Prediction:\", tokenizer.decode(most_likely_token_id))"
   ],
   "metadata": {
    "id": "p0m8jt66aDIh",
    "ExecuteTime": {
     "end_time": "2025-03-24T05:03:56.308913Z",
     "start_time": "2025-03-24T05:03:50.719925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83e5c6bbe47941399ca56557c246cfc6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82bbf97039414138a9ae5f4a3e3f622f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c7018b5c76043de8ee1538f4b895225"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec380687cb5d470889be2c5a2a9f03aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30053251670648ff81f10fc627cb261b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  look\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"The Fermi paradox \"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(end=tokenizer.decode(tokens))\n",
    "line_length = len(tokenizer.decode(tokens))\n",
    "\n",
    "for i in range(500):\n",
    "    # Predict logits with your model\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.as_tensor([tokens]))\n",
    "\n",
    "    # Sample with probabilities\n",
    "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
    "    next_token_index = np.random.choice(len(p_next), p=p_next)\n",
    "\n",
    "    tokens.append(int(next_token_index))\n",
    "    print(end=tokenizer.decode(tokens[-1]))\n",
    "    line_length += len(tokenizer.decode(tokens[-1]))\n",
    "    if line_length > 120:\n",
    "      line_length = 0\n",
    "      print()\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "R8ql3Lo7dXZ2",
    "outputId": "1c1dfcde-751a-4fac-f08c-d03f6686ef3e",
    "ExecuteTime": {
     "end_time": "2025-03-24T05:06:24.014753Z",
     "start_time": "2025-03-24T05:04:56.241859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Fermi paradox  began shortly after on 23 June 1963 while the tail ended up at sundown on 13 June 1963. The scientist\n",
      " Robert Fermi was very surprised. Fermi came back the following morning. He wanted to stay back for another 24 hours but the\n",
      " pressure on him increased and forced him to go to the conference station, having never discussed his theory with his description\n",
      " and later on presented his resolution of the problem for which he was obliged to sign.\n",
      "The Fermi paradox was effected this\n",
      " morning by the Esperanto Jarnassian segregation experiment of the 1965 preference test. The mind of Ernest Murphy provided\n",
      " the previous act   in which he spent 32 hours observing various groups of individuals with visionary states as illusions\n",
      ". Dr. Rachel Ostrom studied Torrey Paine   with a combined mentorship and two realizations as an experienced dreamer of paused\n",
      " dreaming after his own experiment isnabel view was successful.\n",
      "Dr. Murphy had to channell his hips along with his glazed\n",
      " face i.e., head down in four steps\n",
      "Once again Fermi and Jarnassian are paying their own penalty due to their distinct lack\n",
      " of understanding of their awakening experience. Both feel a mental talisman in the lap of their medito, while Jarnassian\n",
      ", meanwhile sees the struggle of realization along with abstract concepts. A fitting contrast to the ineptitude and misrepresent\n",
      "ations in the Esperanto discourse which critics from both around the world seem to all sum up in \"really yum swaats\".\n",
      "DUN\n",
      "NO guarantees meticulous analysis of the punchline: -An NPR interview with Gail Isenberg 61 09-08 2003 by Luke Barrell.\n",
      "NASA\n",
      " (so called for \"unit in the sky\"; experimental leap) A recent study by K. Dennis has found a kind of attitude shift in university\n",
      " students about monogamy, and particularly stopping offering to stay because it feels alien to them. (K. Theoryton, The Metropolitan\n",
      " Museum 1903 ) Cornish, Kent (1998 x citations from Asimov 2009 at http://www.google.com/faculty/cf?id=cl.Macking dialt, Alan\n",
      ", \"The Emotion of a Lecturer in Libertarianism. Pney, p. 234.; http://www.tui.ca/hulto.htm; http://www3istlic.co.uk/quant\n",
      "/a2283.htm; see also Fiat Mastersoff \"August 2nd and Saturday A.M., 2002 Date Perfect"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3NJ0ocgGqop"
   },
   "source": [
    "__Reminder:__ after class, please go to `MaskedSelfAttention.forward` above and finish the job!\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Here's how you can do the same with transformers library"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NTOHu124Gqop",
    "outputId": "5bb38785-a7d9-47e1-a887-c03634945c0b",
    "ExecuteTime": {
     "end_time": "2025-03-24T05:14:24.611844Z",
     "start_time": "2025-03-24T05:14:00.525221Z"
    }
   },
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "print('Generated text:', tokenizer.decode(\n",
    "    model.generate(\n",
    "        **tokenizer(\"The Fermi paradox \", return_tensors='pt'),\n",
    "        do_sample=True, max_new_tokens=50\n",
    "    ).flatten().numpy()\n",
    "))\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1854d8022301428bbc2ca59c48dce84f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2d08b06752b433f9633fc508b4a9520"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  The Fermi paradox  is perhaps the least interesting and least understood phenomena of the Universe .  Because it is a very small world, it cannot be considered a complete vacuum, and so it is not a vacuum we could find with some simple telescopes.  It\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
