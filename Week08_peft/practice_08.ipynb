{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet --upgrade transformers bitsandbytes accelerate sentencepiece optimum auto-gptq\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import BitsAndBytesConfig\nfrom tqdm.auto import tqdm, trange\nassert torch.cuda.is_available(), \"you need cuda for this part\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:25:22.096867Z","iopub.execute_input":"2025-03-08T12:25:22.097046Z","iopub.status.idle":"2025-03-08T12:25:43.340009Z","shell.execute_reply.started":"2025-03-08T12:25:22.097028Z","shell.execute_reply":"2025-03-08T12:25:43.339007Z"},"id":"6dBHFzCjvG_F","outputId":"ec478d7d-fd17-4286-9435-61c3607cdd97"},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:25:53.471693Z","iopub.execute_input":"2025-03-08T12:25:53.472021Z","iopub.status.idle":"2025-03-08T12:25:53.475835Z","shell.execute_reply.started":"2025-03-08T12:25:53.471993Z","shell.execute_reply":"2025-03-08T12:25:53.474915Z"},"id":"eMvtRWLIvG_H"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"model_name = 'Enoch/llama-7b-hf'\n\n# loading Llama tokenizer ...\ntokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# ... and the model itself\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n    quantization_config=quantization_config, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad=False\n\nmodel.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\nmodel.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T13:25:18.372374Z","iopub.execute_input":"2025-03-08T13:25:18.372801Z","iopub.status.idle":"2025-03-08T13:25:18.377042Z","shell.execute_reply.started":"2025-03-08T13:25:18.372761Z","shell.execute_reply":"2025-03-08T13:25:18.376154Z"},"colab":{"referenced_widgets":["56ca25ff81474ff58ce630741f20ff54"]},"id":"PPm8f49IvG_H","outputId":"58f80df0-e911-4221-d624-5fd43b7d7382"},"outputs":[],"execution_count":131},{"cell_type":"code","source":"prompt = 'Tomorrow is the Spring break' # 'A quick brown fox'\nbatch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n\nfor i in range(10):\n    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n\nprint(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:16:26.633133Z","iopub.execute_input":"2025-03-07T18:16:26.633502Z","iopub.status.idle":"2025-03-07T18:16:30.484788Z","shell.execute_reply.started":"2025-03-07T18:16:26.633470Z","shell.execute_reply":"2025-03-07T18:16:30.483782Z"},"id":"Ncw8hMnPvG_I","outputId":"826ce00e-9004-4a31-aa84-06cc30494c98"},"outputs":[{"name":"stdout","text":"\nOutput: <s>Tomorrow is the Spring break for the kids. I am so excited.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class WordEmbeddingsWithLearnedPrompts(nn.Module):\n    \"\"\"\n    Replace model's original word embeddings with a layer that inserts trainable prompts instead of the first N token embeddings.\n    \"\"\"\n    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n        super().__init__()\n        self.original_word_embeddings = word_embeddings\n        self.num_prompts = num_prompts\n        self.learnable_prompts = nn.Parameter(\n            torch.randn(1, num_prompts, word_embeddings.embedding_dim), requires_grad=True\n        )\n\n    def forward(self, input_ids: torch.LongTensor):\n        # Ensure input_ids are of correct type and length\n        assert input_ids.dtype == torch.int64\n        assert input_ids.shape[1] > self.num_prompts, \"Input sequence must be longer than the number of prompts\"\n        assert (input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).all(), \\\n            \"Ensure the first `num_prompts` tokens are PAD tokens\"\n\n        # Embed input_ids\n        embedded_input_ids = self.original_word_embeddings(input_ids)\n\n        # Replace the first `num_prompts` token embeddings with learnable prompts using concatenation\n        prompt_embeds = self.learnable_prompts.expand(input_ids.shape[0], -1, -1)  # [batch_size, num_prompts, embedding_dim]\n        embedded_input_ids = torch.cat([prompt_embeds, embedded_input_ids[:, self.num_prompts:, :]], dim=1)\n\n        return embedded_input_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:21:38.960839Z","iopub.execute_input":"2025-03-07T18:21:38.961235Z","iopub.status.idle":"2025-03-07T18:21:38.967745Z","shell.execute_reply.started":"2025-03-07T18:21:38.961208Z","shell.execute_reply":"2025-03-07T18:21:38.966772Z"},"id":"5VJ7q3mDvG_J"},"outputs":[],"execution_count":8},{"cell_type":"code","source":"num_prompts = 16\ntest_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\ntest_input_ids = tokenizer(\"a cat sat on a mat\", return_tensors='pt')['input_ids'].to(device)\n\nspace_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n                               dtype=torch.int64, device=device)\ntest_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n\nwith torch.amp.autocast('cuda'):\n  test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n\nassert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\nassert test_prompt_embeddings.shape[-1] == model.config.hidden_size\nassert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\nassert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\nprint(\"Looks legit!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:22:10.428888Z","iopub.execute_input":"2025-03-07T18:22:10.429225Z","iopub.status.idle":"2025-03-07T18:22:10.482438Z","shell.execute_reply.started":"2025-03-07T18:22:10.429197Z","shell.execute_reply":"2025-03-07T18:22:10.481694Z"},"id":"SyQU4mqzvG_J","outputId":"9e123670-ed33-498d-9626-682143f41c14"},"outputs":[{"name":"stdout","text":"Looks legit!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n\nmodel.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n\nopt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:22:14.691863Z","iopub.execute_input":"2025-03-07T18:22:14.692218Z","iopub.status.idle":"2025-03-07T18:22:14.697591Z","shell.execute_reply.started":"2025-03-07T18:22:14.692190Z","shell.execute_reply":"2025-03-07T18:22:14.696847Z"},"id":"_DRhGGDGvG_J"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"the_truth = \"Tomorrow is the Spring break, I will miss the school!\" #\"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\nbatch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\nspace_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n                               dtype=torch.int64, device=device)\nbatch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\nbatch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n\noutputs = model(**batch)\nnext_word_logits = outputs.logits[:, num_prompts : -1, :]\ntrue_next_tokens = batch['input_ids'][:, num_prompts + 1:]\nloss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\nprint(\"Loss:\", loss)\nscaler = torch.amp.GradScaler('cuda')\n\nloss_threshold = 0.1\nepoch = 0\n\nwhile True:\n    opt.zero_grad()\n\n    with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n        outputs = model(**batch)\n        next_word_logits = outputs.logits[:, num_prompts:-1, :]\n        true_next_tokens = batch['input_ids'][:, num_prompts+1:]\n        loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n\n    # Backpropagate using mixed precision\n    scaler.scale(loss).backward()\n    scaler.step(opt)\n    scaler.update()\n\n    print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n\n    if loss.item() <= loss_threshold:\n        break\n\n    epoch += 1\n\nassert loss.item() <= 0.1\nprint(\"Good job!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T13:24:52.987862Z","iopub.execute_input":"2025-03-08T13:24:52.988176Z","iopub.status.idle":"2025-03-08T13:24:52.992020Z","shell.execute_reply.started":"2025-03-08T13:24:52.988152Z","shell.execute_reply":"2025-03-08T13:24:52.991179Z"},"id":"FZzFGOP_vG_J"},"outputs":[],"execution_count":130},{"cell_type":"code","source":"prompt = 'Tomorrow is the Spring break' # 'A quick brown fox'\nbatch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\nbatch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\nbatch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n\n\nfor i in range(17):\n    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n\nprint(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist()))\n\n# if you did everything right, the model will deny that the fox jumped over the lazy dog","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:24:56.796443Z","iopub.execute_input":"2025-03-07T18:24:56.796795Z","iopub.status.idle":"2025-03-07T18:25:03.777847Z","shell.execute_reply.started":"2025-03-07T18:24:56.796769Z","shell.execute_reply":"2025-03-07T18:25:03.777103Z"},"id":"RTwtq0NvvG_K"},"outputs":[{"name":"stdout","text":"\nOutput: <s>Tomorrow is the Spring break, I will miss the school!\nThe school is closed, I will miss the\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# for name, layer in model.model.layers.named_modules():\n#     if isinstance(layer, torch.nn.Linear):\n#         print(name, layer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T13:43:43.735381Z","iopub.execute_input":"2025-03-07T13:43:43.735682Z","iopub.status.idle":"2025-03-07T13:43:43.795662Z","shell.execute_reply.started":"2025-03-07T13:43:43.735657Z","shell.execute_reply":"2025-03-07T13:43:43.794995Z"},"id":"rN8zPcjBvG_K"},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Reload the model!","metadata":{}},{"cell_type":"code","source":"model_name = 'Enoch/llama-7b-hf'\n\n# loading Llama tokenizer ...\ntokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# ... and the model itself\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n    quantization_config=quantization_config, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad=False\n\nmodel.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\nmodel.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:28:06.093695Z","iopub.execute_input":"2025-03-08T12:28:06.094292Z","iopub.status.idle":"2025-03-08T12:28:06.451405Z","shell.execute_reply.started":"2025-03-08T12:28:06.094260Z","shell.execute_reply":"2025-03-08T12:28:06.450616Z"},"id":"QANfUlP8vG_K"},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from peft import LoraConfig, TaskType","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:28:06.453064Z","iopub.execute_input":"2025-03-08T12:28:06.453388Z","iopub.status.idle":"2025-03-08T12:28:06.456889Z","shell.execute_reply.started":"2025-03-08T12:28:06.453356Z","shell.execute_reply":"2025-03-08T12:28:06.455999Z"},"id":"J4T8bOa4vG_K"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#model.config #, tokenizer)","metadata":{"trusted":true,"id":"3GXZ4goYvG_K","execution":{"iopub.status.busy":"2025-03-07T18:27:32.476996Z","iopub.execute_input":"2025-03-07T18:27:32.477301Z","iopub.status.idle":"2025-03-07T18:27:32.480965Z","shell.execute_reply.started":"2025-03-07T18:27:32.477280Z","shell.execute_reply":"2025-03-07T18:27:32.479994Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"peft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,  # GPT-like models\n    r=8,  # Low-rank dimension\n    lora_alpha=32,  # Scaling factor for initialization\n    lora_dropout=0.1,  # Dropout probability\n    #target_modules=[\"q_proj\", \"v_proj\"]  # Applies LoRA only to key transformer layers\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T12:28:06.457690Z","iopub.execute_input":"2025-03-08T12:28:06.458025Z","iopub.status.idle":"2025-03-08T12:28:06.470416Z","shell.execute_reply.started":"2025-03-08T12:28:06.457988Z","shell.execute_reply":"2025-03-08T12:28:06.469700Z"},"id":"Y3a7pL9ivG_K"},"outputs":[],"execution_count":7},{"cell_type":"code","source":"peft_model = peft.get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T13:20:17.118757Z","iopub.execute_input":"2025-03-08T13:20:17.119099Z","iopub.status.idle":"2025-03-08T13:20:17.215897Z","shell.execute_reply.started":"2025-03-08T13:20:17.119068Z","shell.execute_reply":"2025-03-08T13:20:17.214963Z"},"id":"GW8PpsTxvG_K"},"outputs":[],"execution_count":117},{"cell_type":"code","source":"#peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T13:20:17.385434Z","iopub.execute_input":"2025-03-08T13:20:17.385699Z","iopub.status.idle":"2025-03-08T13:20:17.388990Z","shell.execute_reply.started":"2025-03-08T13:20:17.385679Z","shell.execute_reply":"2025-03-08T13:20:17.388240Z"},"id":"qxWM6K2GvG_K"},"outputs":[],"execution_count":118},{"cell_type":"code","source":"in_prompt = \"Tomorrow is the Spring break\"\nout_prompt = \"Tomorrow is the Spring break, I will miss the school!\"\n# Move input and target tensors to device\nin_token_idx = tokenizer([in_prompt], return_tensors='pt')\nout_token_idx = tokenizer([out_prompt], return_tensors='pt')\n\nin_token_idx = {k: v.to(device) for k, v in in_token_idx.items()}\nout_token_idx = {k: v.to(device) for k, v in out_token_idx.items()}\n\n# Labels: Clone target and mask the prompt portion\nlabels = out_token_idx[\"input_ids\"].clone()\nlabels[:, :in_token_idx[\"input_ids\"].shape[1]] = -100  # Ignore prompt for loss\n\n# Training input: concatenate prompt with the target tokens\nin_token_idx[\"input_ids\"] = out_token_idx[\"input_ids\"]\nin_token_idx[\"attention_mask\"] = out_token_idx[\"attention_mask\"]\nin_token_idx, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T13:20:17.873358Z","iopub.execute_input":"2025-03-08T13:20:17.873667Z","iopub.status.idle":"2025-03-08T13:20:17.884096Z","shell.execute_reply.started":"2025-03-08T13:20:17.873640Z","shell.execute_reply":"2025-03-08T13:20:17.883410Z"}},"outputs":[{"execution_count":119,"output_type":"execute_result","data":{"text/plain":"({'input_ids': tensor([[    1,  4335, 22396,   338,   278,  7206,  2867, 29892,   306,   674,\n            3052,   278,  3762, 29991]], device='cuda:0'),\n  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')},\n tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100, 29892,   306,   674,\n           3052,   278,  3762, 29991]], device='cuda:0'))"},"metadata":{}}],"execution_count":119},{"cell_type":"code","source":"with torch.amp.autocast('cuda'):\n    out = peft_model(**in_token_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T13:20:20.937582Z","iopub.execute_input":"2025-03-08T13:20:20.937870Z","iopub.status.idle":"2025-03-08T13:20:21.238771Z","shell.execute_reply.started":"2025-03-08T13:20:20.937848Z","shell.execute_reply":"2025-03-08T13:20:21.237805Z"},"id":"L0hvQy4FvG_L"},"outputs":[],"execution_count":120},{"cell_type":"code","source":"out.logits.size(), labels.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T13:20:22.242288Z","iopub.execute_input":"2025-03-08T13:20:22.242570Z","iopub.status.idle":"2025-03-08T13:20:22.247538Z","shell.execute_reply.started":"2025-03-08T13:20:22.242549Z","shell.execute_reply":"2025-03-08T13:20:22.246669Z"},"id":"W-4x3UEmvG_L","outputId":"46c00677-c6e4-4de4-e00d-7d1b767a3de1"},"outputs":[{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"(torch.Size([1, 14, 32000]), torch.Size([1, 14]))"},"metadata":{}}],"execution_count":121},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(peft_model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T13:20:24.676745Z","iopub.execute_input":"2025-03-08T13:20:24.677030Z","iopub.status.idle":"2025-03-08T13:20:24.685858Z","shell.execute_reply.started":"2025-03-08T13:20:24.677008Z","shell.execute_reply":"2025-03-08T13:20:24.684942Z"},"id":"V5bTnLFPvG_M"},"outputs":[],"execution_count":122},{"cell_type":"code","source":"peft_model.train()\nscaler = torch.amp.GradScaler('cuda')\nloss_threshold = 0.1\nepoch = 0\n\nwhile True:\n    optimizer.zero_grad()\n\n    with torch.amp.autocast(device_type='cuda'):\n        out = peft_model(**in_token_idx)\n        logits = out.logits[:,:-1,:] #[:, prompt_length:, :]\n        loss = F.cross_entropy(logits.flatten(0, 1), labels[:,1:].flatten(0, 1))\n\n    # Backpropagate using mixed precision\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n    print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n\n    if loss.item() <= loss_threshold:\n        break\n\n    epoch += 1\n\nassert loss.item() <= 0.1\nprint(\"Good job!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T13:24:37.093087Z","iopub.execute_input":"2025-03-08T13:24:37.093442Z","iopub.status.idle":"2025-03-08T13:24:37.096884Z","shell.execute_reply.started":"2025-03-08T13:24:37.093415Z","shell.execute_reply":"2025-03-08T13:24:37.096109Z"},"id":"E-2ypwg_vG_M"},"outputs":[],"execution_count":129},{"cell_type":"code","source":"peft_model.eval()\nprompt = 'Tomorrow is the Spring break' # 'A quick brown fox'\n\nbatch = tokenizer([prompt], return_tensors='pt').to(device)\n\nfor i in range(17):\n    next_token = peft_model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n\nprint(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, :].cpu().numpy().tolist()))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T13:20:53.999628Z","iopub.execute_input":"2025-03-08T13:20:53.999916Z","iopub.status.idle":"2025-03-08T13:20:59.427426Z","shell.execute_reply.started":"2025-03-08T13:20:53.999881Z","shell.execute_reply":"2025-03-08T13:20:59.426647Z"},"id":"Uga3h79wvG_M"},"outputs":[{"name":"stdout","text":"\nOutput: <s>Tomorrow is the Spring break, I will miss the school!\nI will miss the school!\nI will\n","output_type":"stream"}],"execution_count":124},{"cell_type":"code","source":"generated_tokens = peft_model.generate(batch[\"input_ids\"])\ntokenizer.decode(generated_tokens[0], skip_special_tokens=True)","metadata":{"trusted":true,"id":"iPxupa4-vG_M","execution":{"iopub.status.busy":"2025-03-08T13:23:22.509543Z","iopub.execute_input":"2025-03-08T13:23:22.509883Z","iopub.status.idle":"2025-03-08T13:23:24.467420Z","shell.execute_reply.started":"2025-03-08T13:23:22.509853Z","shell.execute_reply":"2025-03-08T13:23:24.466521Z"}},"outputs":[{"execution_count":128,"output_type":"execute_result","data":{"text/plain":"'Tomorrow is the Spring break, I will miss the school!\\nI will miss the school!\\nI will miss the school! I will miss the school! I will miss the school! I will miss the'"},"metadata":{}}],"execution_count":128},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}