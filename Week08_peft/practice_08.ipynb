{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install --quiet --upgrade transformers bitsandbytes accelerate sentencepiece optimum auto-gptq\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import BitsAndBytesConfig\nfrom tqdm.auto import tqdm, trange\nassert torch.cuda.is_available(), \"you need cuda for this part\"",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T12:25:22.096867Z",
     "iopub.execute_input": "2025-03-08T12:25:22.097046Z",
     "iopub.status.idle": "2025-03-08T12:25:43.340009Z",
     "shell.execute_reply.started": "2025-03-08T12:25:22.097028Z",
     "shell.execute_reply": "2025-03-08T12:25:43.339007Z"
    },
    "id": "6dBHFzCjvG_F",
    "outputId": "ec478d7d-fd17-4286-9435-61c3607cdd97",
    "ExecuteTime": {
     "end_time": "2025-03-31T20:27:37.661806Z",
     "start_time": "2025-03-31T20:27:31.659052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\r\n",
      "You should consider upgrading via the '/users/u29/bobbyd/RIT_LLM/.venv/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T12:25:53.471693Z",
     "iopub.execute_input": "2025-03-08T12:25:53.472021Z",
     "iopub.status.idle": "2025-03-08T12:25:53.475835Z",
     "shell.execute_reply.started": "2025-03-08T12:25:53.471993Z",
     "shell.execute_reply": "2025-03-08T12:25:53.474915Z"
    },
    "id": "eMvtRWLIvG_H",
    "ExecuteTime": {
     "end_time": "2025-03-31T20:31:57.227858Z",
     "start_time": "2025-03-31T20:31:57.224222Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "model_name = 'Enoch/llama-7b-hf'\n\n# loading Llama tokenizer ...\ntokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# ... and the model itself\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n    quantization_config=quantization_config, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad=False\n\nmodel.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\nmodel.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T13:25:18.372374Z",
     "iopub.execute_input": "2025-03-08T13:25:18.372801Z",
     "iopub.status.idle": "2025-03-08T13:25:18.377042Z",
     "shell.execute_reply.started": "2025-03-08T13:25:18.372761Z",
     "shell.execute_reply": "2025-03-08T13:25:18.376154Z"
    },
    "colab": {
     "referenced_widgets": [
      "56ca25ff81474ff58ce630741f20ff54"
     ]
    },
    "id": "PPm8f49IvG_H",
    "outputId": "58f80df0-e911-4221-d624-5fd43b7d7382",
    "ExecuteTime": {
     "end_time": "2025-03-31T20:32:42.133288Z",
     "start_time": "2025-03-31T20:32:28.095334Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ba8e023dea84265ab17b628740dca2f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "prompt = 'Tomorrow is the Spring break' # 'A quick brown fox'\nbatch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n\nfor i in range(10):\n    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n\nprint(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-07T18:16:26.633133Z",
     "iopub.execute_input": "2025-03-07T18:16:26.633502Z",
     "iopub.status.idle": "2025-03-07T18:16:30.484788Z",
     "shell.execute_reply.started": "2025-03-07T18:16:26.633470Z",
     "shell.execute_reply": "2025-03-07T18:16:30.483782Z"
    },
    "id": "Ncw8hMnPvG_I",
    "outputId": "826ce00e-9004-4a31-aa84-06cc30494c98",
    "ExecuteTime": {
     "end_time": "2025-03-31T20:37:58.588631Z",
     "start_time": "2025-03-31T20:37:57.182134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: <s>Tomorrow is the Spring break for the kids. I am so excited.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "class WordEmbeddingsWithLearnedPrompts(nn.Module):\n    \"\"\"\n    Replace model's original word embeddings with a layer that inserts trainable prompts instead of the first N token embeddings.\n    \"\"\"\n    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n        super().__init__()\n        self.original_word_embeddings = word_embeddings\n        self.num_prompts = num_prompts\n        self.learnable_prompts = nn.Parameter(\n            torch.randn(1, num_prompts, word_embeddings.embedding_dim), requires_grad=True\n        )\n\n    def forward(self, input_ids: torch.LongTensor):\n        # Ensure input_ids are of correct type and length\n        assert input_ids.dtype == torch.int64\n        assert input_ids.shape[1] > self.num_prompts, \"Input sequence must be longer than the number of prompts\"\n        assert (input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).all(), \\\n            \"Ensure the first `num_prompts` tokens are PAD tokens\"\n\n        # Embed input_ids\n        embedded_input_ids = self.original_word_embeddings(input_ids)\n\n        # Replace the first `num_prompts` token embeddings with learnable prompts using concatenation\n        prompt_embeds = self.learnable_prompts.expand(input_ids.shape[0], -1, -1)  # [batch_size, num_prompts, embedding_dim]\n        embedded_input_ids = torch.cat([prompt_embeds, embedded_input_ids[:, self.num_prompts:, :]], dim=1)\n\n        return embedded_input_ids",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-07T18:21:38.960839Z",
     "iopub.execute_input": "2025-03-07T18:21:38.961235Z",
     "iopub.status.idle": "2025-03-07T18:21:38.967745Z",
     "shell.execute_reply.started": "2025-03-07T18:21:38.961208Z",
     "shell.execute_reply": "2025-03-07T18:21:38.966772Z"
    },
    "id": "5VJ7q3mDvG_J",
    "ExecuteTime": {
     "end_time": "2025-03-31T20:38:00.169618Z",
     "start_time": "2025-03-31T20:38:00.163453Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "num_prompts = 16\ntest_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\ntest_input_ids = tokenizer(\"a cat sat on a mat\", return_tensors='pt')['input_ids'].to(device)\n\nspace_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n                               dtype=torch.int64, device=device)\ntest_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n\nwith torch.amp.autocast('cuda'):\n  test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n\nassert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\nassert test_prompt_embeddings.shape[-1] == model.config.hidden_size\nassert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\nassert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\nprint(\"Looks legit!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-07T18:22:10.428888Z",
     "iopub.execute_input": "2025-03-07T18:22:10.429225Z",
     "iopub.status.idle": "2025-03-07T18:22:10.482438Z",
     "shell.execute_reply.started": "2025-03-07T18:22:10.429197Z",
     "shell.execute_reply": "2025-03-07T18:22:10.481694Z"
    },
    "id": "SyQU4mqzvG_J",
    "outputId": "9e123670-ed33-498d-9626-682143f41c14",
    "ExecuteTime": {
     "end_time": "2025-03-31T20:38:03.270714Z",
     "start_time": "2025-03-31T20:38:03.261324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks legit!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n\nmodel.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n\nopt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-07T18:22:14.691863Z",
     "iopub.execute_input": "2025-03-07T18:22:14.692218Z",
     "iopub.status.idle": "2025-03-07T18:22:14.697591Z",
     "shell.execute_reply.started": "2025-03-07T18:22:14.692190Z",
     "shell.execute_reply": "2025-03-07T18:22:14.696847Z"
    },
    "id": "_DRhGGDGvG_J",
    "ExecuteTime": {
     "end_time": "2025-03-25T22:31:45.683560Z",
     "start_time": "2025-03-25T22:31:45.679439Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "the_truth = \"Tomorrow is the Spring break, I will miss the school!\" #\"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\nbatch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\nspace_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n                               dtype=torch.int64, device=device)\nbatch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\nbatch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n\noutputs = model(**batch)\nnext_word_logits = outputs.logits[:, num_prompts : -1, :]\ntrue_next_tokens = batch['input_ids'][:, num_prompts + 1:]\nloss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\nprint(\"Loss:\", loss)\nscaler = torch.amp.GradScaler('cuda')\n\nloss_threshold = 0.1\nepoch = 0\n\nwhile True:\n    opt.zero_grad()\n\n    with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n        outputs = model(**batch)\n        next_word_logits = outputs.logits[:, num_prompts:-1, :]\n        true_next_tokens = batch['input_ids'][:, num_prompts+1:]\n        loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n\n    # Backpropagate using mixed precision\n    scaler.scale(loss).backward()\n    scaler.step(opt)\n    scaler.update()\n\n    print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n\n    if loss.item() <= loss_threshold:\n        break\n\n    epoch += 1\n\nassert loss.item() <= 0.1\nprint(\"Good job!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T13:24:52.987862Z",
     "iopub.execute_input": "2025-03-08T13:24:52.988176Z",
     "iopub.status.idle": "2025-03-08T13:24:52.992020Z",
     "shell.execute_reply.started": "2025-03-08T13:24:52.988152Z",
     "shell.execute_reply": "2025-03-08T13:24:52.991179Z"
    },
    "id": "FZzFGOP_vG_J",
    "ExecuteTime": {
     "end_time": "2025-03-25T22:32:07.484379Z",
     "start_time": "2025-03-25T22:31:54.561548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(7.6555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0: Loss = 7.653395652770996\n",
      "Epoch 1: Loss = 7.653395652770996\n",
      "Epoch 2: Loss = 7.653395652770996\n",
      "Epoch 3: Loss = 7.653395652770996\n",
      "Epoch 4: Loss = 7.09840726852417\n",
      "Epoch 5: Loss = 6.675856590270996\n",
      "Epoch 6: Loss = 6.310020923614502\n",
      "Epoch 7: Loss = 5.953537940979004\n",
      "Epoch 8: Loss = 5.669583797454834\n",
      "Epoch 9: Loss = 5.462852954864502\n",
      "Epoch 10: Loss = 5.237905502319336\n",
      "Epoch 11: Loss = 5.023512840270996\n",
      "Epoch 12: Loss = 4.772573471069336\n",
      "Epoch 13: Loss = 4.561899185180664\n",
      "Epoch 14: Loss = 4.367037296295166\n",
      "Epoch 15: Loss = 4.18359375\n",
      "Epoch 16: Loss = 4.008939266204834\n",
      "Epoch 17: Loss = 3.808668851852417\n",
      "Epoch 18: Loss = 3.598182201385498\n",
      "Epoch 19: Loss = 3.403245210647583\n",
      "Epoch 20: Loss = 3.196026086807251\n",
      "Epoch 21: Loss = 2.989633321762085\n",
      "Epoch 22: Loss = 2.781024694442749\n",
      "Epoch 23: Loss = 2.56689453125\n",
      "Epoch 24: Loss = 2.376840353012085\n",
      "Epoch 25: Loss = 2.185246467590332\n",
      "Epoch 26: Loss = 2.009840726852417\n",
      "Epoch 27: Loss = 1.8397122621536255\n",
      "Epoch 28: Loss = 1.674363374710083\n",
      "Epoch 29: Loss = 1.521240234375\n",
      "Epoch 30: Loss = 1.3703659772872925\n",
      "Epoch 31: Loss = 1.234905481338501\n",
      "Epoch 32: Loss = 1.104351282119751\n",
      "Epoch 33: Loss = 0.9770789742469788\n",
      "Epoch 34: Loss = 0.8591543436050415\n",
      "Epoch 35: Loss = 0.750535249710083\n",
      "Epoch 36: Loss = 0.6473411917686462\n",
      "Epoch 37: Loss = 0.5509666800498962\n",
      "Epoch 38: Loss = 0.46063467860221863\n",
      "Epoch 39: Loss = 0.37796491384506226\n",
      "Epoch 40: Loss = 0.3069469630718231\n",
      "Epoch 41: Loss = 0.24476975202560425\n",
      "Epoch 42: Loss = 0.19245558977127075\n",
      "Epoch 43: Loss = 0.14984482526779175\n",
      "Epoch 44: Loss = 0.11662410199642181\n",
      "Epoch 45: Loss = 0.09263786673545837\n",
      "Good job!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "prompt = 'Tomorrow is the Spring break' # 'A quick brown fox'\nbatch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\nbatch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\nbatch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n\n\nfor i in range(17):\n    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n\nprint(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist()))\n\n# if you did everything right, the model will deny that the fox jumped over the lazy dog",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-07T18:24:56.796443Z",
     "iopub.execute_input": "2025-03-07T18:24:56.796795Z",
     "iopub.status.idle": "2025-03-07T18:25:03.777847Z",
     "shell.execute_reply.started": "2025-03-07T18:24:56.796769Z",
     "shell.execute_reply": "2025-03-07T18:25:03.777103Z"
    },
    "id": "RTwtq0NvvG_K",
    "ExecuteTime": {
     "end_time": "2025-03-25T22:35:52.718182Z",
     "start_time": "2025-03-25T22:35:50.910603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: <s>Tomorrow is the Spring break, I will miss the school! The! The! The! The! The!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "# for name, layer in model.model.layers.named_modules():\n#     if isinstance(layer, torch.nn.Linear):\n#         print(name, layer)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-07T13:43:43.735381Z",
     "iopub.execute_input": "2025-03-07T13:43:43.735682Z",
     "iopub.status.idle": "2025-03-07T13:43:43.795662Z",
     "shell.execute_reply.started": "2025-03-07T13:43:43.735657Z",
     "shell.execute_reply": "2025-03-07T13:43:43.794995Z"
    },
    "id": "rN8zPcjBvG_K"
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "Reload the model!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model_name = 'Enoch/llama-7b-hf'\n\n# loading Llama tokenizer ...\ntokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# ... and the model itself\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n    quantization_config=quantization_config, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad=False\n\nmodel.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\nmodel.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-25T22:36:48.627111Z",
     "start_time": "2025-03-25T22:36:35.894828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e45b9254df7f479b814e39e1cbcd2cae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "import peft",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T12:28:06.093695Z",
     "iopub.execute_input": "2025-03-08T12:28:06.094292Z",
     "iopub.status.idle": "2025-03-08T12:28:06.451405Z",
     "shell.execute_reply.started": "2025-03-08T12:28:06.094260Z",
     "shell.execute_reply": "2025-03-08T12:28:06.450616Z"
    },
    "id": "QANfUlP8vG_K",
    "ExecuteTime": {
     "end_time": "2025-03-25T22:37:29.167272Z",
     "start_time": "2025-03-25T22:37:27.880915Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "from peft import LoraConfig, TaskType",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T12:28:06.453064Z",
     "iopub.execute_input": "2025-03-08T12:28:06.453388Z",
     "iopub.status.idle": "2025-03-08T12:28:06.456889Z",
     "shell.execute_reply.started": "2025-03-08T12:28:06.453356Z",
     "shell.execute_reply": "2025-03-08T12:28:06.455999Z"
    },
    "id": "J4T8bOa4vG_K",
    "ExecuteTime": {
     "end_time": "2025-03-25T22:37:46.245307Z",
     "start_time": "2025-03-25T22:37:46.242414Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "#model.config #, tokenizer)",
   "metadata": {
    "trusted": true,
    "id": "3GXZ4goYvG_K",
    "execution": {
     "iopub.status.busy": "2025-03-07T18:27:32.476996Z",
     "iopub.execute_input": "2025-03-07T18:27:32.477301Z",
     "iopub.status.idle": "2025-03-07T18:27:32.480965Z",
     "shell.execute_reply.started": "2025-03-07T18:27:32.477280Z",
     "shell.execute_reply": "2025-03-07T18:27:32.479994Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "peft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,  # GPT-like models\n    r=8,  # Low-rank dimension\n    lora_alpha=32,  # Scaling factor for initialization\n    lora_dropout=0.1,  # Dropout probability\n    #target_modules=[\"q_proj\", \"v_proj\"]  # Applies LoRA only to key transformer layers\n)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T12:28:06.457690Z",
     "iopub.execute_input": "2025-03-08T12:28:06.458025Z",
     "iopub.status.idle": "2025-03-08T12:28:06.470416Z",
     "shell.execute_reply.started": "2025-03-08T12:28:06.457988Z",
     "shell.execute_reply": "2025-03-08T12:28:06.469700Z"
    },
    "id": "Y3a7pL9ivG_K",
    "ExecuteTime": {
     "end_time": "2025-03-25T22:37:48.388108Z",
     "start_time": "2025-03-25T22:37:48.384228Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "peft_model = peft.get_peft_model(model, peft_config)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T13:20:17.118757Z",
     "iopub.execute_input": "2025-03-08T13:20:17.119099Z",
     "iopub.status.idle": "2025-03-08T13:20:17.215897Z",
     "shell.execute_reply.started": "2025-03-08T13:20:17.119068Z",
     "shell.execute_reply": "2025-03-08T13:20:17.214963Z"
    },
    "id": "GW8PpsTxvG_K",
    "ExecuteTime": {
     "end_time": "2025-03-25T22:37:50.109194Z",
     "start_time": "2025-03-25T22:37:49.991655Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "#peft_model",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T13:20:17.385434Z",
     "iopub.execute_input": "2025-03-08T13:20:17.385699Z",
     "iopub.status.idle": "2025-03-08T13:20:17.388990Z",
     "shell.execute_reply.started": "2025-03-08T13:20:17.385679Z",
     "shell.execute_reply": "2025-03-08T13:20:17.388240Z"
    },
    "id": "qxWM6K2GvG_K"
   },
   "outputs": [],
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "source": "in_prompt = \"Tomorrow is the Spring break\"\nout_prompt = \"Tomorrow is the Spring break, I will miss the school!\"\n# Move input and target tensors to device\nin_token_idx = tokenizer([in_prompt], return_tensors='pt')\nout_token_idx = tokenizer([out_prompt], return_tensors='pt')\n\nin_token_idx = {k: v.to(device) for k, v in in_token_idx.items()}\nout_token_idx = {k: v.to(device) for k, v in out_token_idx.items()}\n\n# Labels: Clone target and mask the prompt portion\nlabels = out_token_idx[\"input_ids\"].clone()\nlabels[:, :in_token_idx[\"input_ids\"].shape[1]] = -100  # Ignore prompt for loss\n\n# Training input: concatenate prompt with the target tokens\nin_token_idx[\"input_ids\"] = out_token_idx[\"input_ids\"]\nin_token_idx[\"attention_mask\"] = out_token_idx[\"attention_mask\"]\nin_token_idx, labels",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T13:20:17.873358Z",
     "iopub.execute_input": "2025-03-08T13:20:17.873667Z",
     "iopub.status.idle": "2025-03-08T13:20:17.884096Z",
     "shell.execute_reply.started": "2025-03-08T13:20:17.873640Z",
     "shell.execute_reply": "2025-03-08T13:20:17.883410Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-25T22:37:53.035939Z",
     "start_time": "2025-03-25T22:37:53.024761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[    1,  4335, 22396,   338,   278,  7206,  2867, 29892,   306,   674,\n",
       "            3052,   278,  3762, 29991]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')},\n",
       " tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100, 29892,   306,   674,\n",
       "           3052,   278,  3762, 29991]], device='cuda:0'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "with torch.amp.autocast('cuda'):\n    out = peft_model(**in_token_idx)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T13:20:20.937582Z",
     "iopub.execute_input": "2025-03-08T13:20:20.937870Z",
     "iopub.status.idle": "2025-03-08T13:20:21.238771Z",
     "shell.execute_reply.started": "2025-03-08T13:20:20.937848Z",
     "shell.execute_reply": "2025-03-08T13:20:21.237805Z"
    },
    "id": "L0hvQy4FvG_L",
    "ExecuteTime": {
     "end_time": "2025-03-25T22:38:01.108930Z",
     "start_time": "2025-03-25T22:38:00.992494Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/u29/bobbyd/RIT_LLM/.venv/lib64/python3.9/site-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "out.logits.size(), labels.size()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T13:20:22.242288Z",
     "iopub.execute_input": "2025-03-08T13:20:22.242570Z",
     "iopub.status.idle": "2025-03-08T13:20:22.247538Z",
     "shell.execute_reply.started": "2025-03-08T13:20:22.242549Z",
     "shell.execute_reply": "2025-03-08T13:20:22.246669Z"
    },
    "id": "W-4x3UEmvG_L",
    "outputId": "46c00677-c6e4-4de4-e00d-7d1b767a3de1",
    "ExecuteTime": {
     "end_time": "2025-03-25T22:38:09.362805Z",
     "start_time": "2025-03-25T22:38:09.358544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 14, 32000]), torch.Size([1, 14]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "optimizer = torch.optim.AdamW(peft_model.parameters(), lr=1e-4)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T13:20:24.676745Z",
     "iopub.execute_input": "2025-03-08T13:20:24.677030Z",
     "iopub.status.idle": "2025-03-08T13:20:24.685858Z",
     "shell.execute_reply.started": "2025-03-08T13:20:24.677008Z",
     "shell.execute_reply": "2025-03-08T13:20:24.684942Z"
    },
    "id": "V5bTnLFPvG_M",
    "ExecuteTime": {
     "end_time": "2025-03-25T22:38:11.289332Z",
     "start_time": "2025-03-25T22:38:11.283349Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": "peft_model.train()\nscaler = torch.amp.GradScaler('cuda')\nloss_threshold = 0.1\nepoch = 0\n\nwhile True:\n    optimizer.zero_grad()\n\n    with torch.amp.autocast(device_type='cuda'):\n        out = peft_model(**in_token_idx)\n        logits = out.logits[:,:-1,:] #[:, prompt_length:, :]\n        loss = F.cross_entropy(logits.flatten(0, 1), labels[:,1:].flatten(0, 1))\n\n    # Backpropagate using mixed precision\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n    print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n\n    if loss.item() <= loss_threshold:\n        break\n\n    epoch += 1\n\nassert loss.item() <= 0.1\nprint(\"Good job!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T13:24:37.093087Z",
     "iopub.execute_input": "2025-03-08T13:24:37.093442Z",
     "iopub.status.idle": "2025-03-08T13:24:37.096884Z",
     "shell.execute_reply.started": "2025-03-08T13:24:37.093415Z",
     "shell.execute_reply": "2025-03-08T13:24:37.096109Z"
    },
    "id": "E-2ypwg_vG_M",
    "ExecuteTime": {
     "end_time": "2025-03-25T22:38:23.919431Z",
     "start_time": "2025-03-25T22:38:13.402404Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 3.567661762237549\n",
      "Epoch 1: Loss = 3.567661762237549\n",
      "Epoch 2: Loss = 3.567661762237549\n",
      "Epoch 3: Loss = 3.346400737762451\n",
      "Epoch 4: Loss = 3.1573660373687744\n",
      "Epoch 5: Loss = 2.9093191623687744\n",
      "Epoch 6: Loss = 2.6485769748687744\n",
      "Epoch 7: Loss = 2.4174106121063232\n",
      "Epoch 8: Loss = 2.1909878253936768\n",
      "Epoch 9: Loss = 1.94384765625\n",
      "Epoch 10: Loss = 1.6928013563156128\n",
      "Epoch 11: Loss = 1.5080217123031616\n",
      "Epoch 12: Loss = 1.32373046875\n",
      "Epoch 13: Loss = 1.1727644205093384\n",
      "Epoch 14: Loss = 1.0101492404937744\n",
      "Epoch 15: Loss = 0.8516497015953064\n",
      "Epoch 16: Loss = 0.6873212456703186\n",
      "Epoch 17: Loss = 0.5632607340812683\n",
      "Epoch 18: Loss = 0.4746333658695221\n",
      "Epoch 19: Loss = 0.3963884711265564\n",
      "Epoch 20: Loss = 0.3099321722984314\n",
      "Epoch 21: Loss = 0.2353123277425766\n",
      "Epoch 22: Loss = 0.1674695760011673\n",
      "Epoch 23: Loss = 0.1191515251994133\n",
      "Epoch 24: Loss = 0.0858415886759758\n",
      "Good job!\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "peft_model.eval()\nprompt = 'Tomorrow is the Spring break' # 'A quick brown fox'\n\nbatch = tokenizer([prompt], return_tensors='pt').to(device)\n\nfor i in range(17):\n    next_token = peft_model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n\nprint(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, :].cpu().numpy().tolist()))\n\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-03-08T13:20:53.999628Z",
     "iopub.execute_input": "2025-03-08T13:20:53.999916Z",
     "iopub.status.idle": "2025-03-08T13:20:59.427426Z",
     "shell.execute_reply.started": "2025-03-08T13:20:53.999881Z",
     "shell.execute_reply": "2025-03-08T13:20:59.426647Z"
    },
    "id": "Uga3h79wvG_M",
    "ExecuteTime": {
     "end_time": "2025-03-25T22:38:49.060569Z",
     "start_time": "2025-03-25T22:38:47.278471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: <s>Tomorrow is the Spring break, I will miss the school!\n",
      "I will miss the school!\n",
      "I will\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": "generated_tokens = peft_model.generate(batch[\"input_ids\"])\ntokenizer.decode(generated_tokens[0], skip_special_tokens=True)",
   "metadata": {
    "trusted": true,
    "id": "iPxupa4-vG_M",
    "execution": {
     "iopub.status.busy": "2025-03-08T13:23:22.509543Z",
     "iopub.execute_input": "2025-03-08T13:23:22.509883Z",
     "iopub.status.idle": "2025-03-08T13:23:24.467420Z",
     "shell.execute_reply.started": "2025-03-08T13:23:22.509853Z",
     "shell.execute_reply": "2025-03-08T13:23:24.466521Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-25T22:38:52.626637Z",
     "start_time": "2025-03-25T22:38:51.579113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tomorrow is the Spring break, I will miss the school!\\nI will miss the school!\\nI will miss the school! I will miss the school! I will miss the school! I will miss the'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
