{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30887,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Homework 5 (10pt): Question search engine\n",
    "\n",
    "Remeber Week01, where you used GloVe embeddings to find related questions? That was... cute. Now, it's time to really solve this task using context-aware embeddings.\n",
    "\n",
    "__Warning:__ this task assumes you have seen `practice06.ipynb` [notebook](https://github.com/anton-selitskiy/RIT_LLM/blob/main/Week06_bert/practice06.ipynb)\n",
    "\n",
    "This assignmend is inspired by this [notebook](https://github.com/yandexdataschool/nlp_course/blob/2024/week05_transfer/homework.ipynb)"
   ],
   "metadata": {
    "id": "Wlu-s2k9D1Ba"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# %pip install --upgrade transformers datasets accelerate deepspeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import datasets"
   ],
   "metadata": {
    "id": "HYffoHiI8du5",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T23:32:24.677259Z",
     "iopub.execute_input": "2025-02-18T23:32:24.677572Z",
     "iopub.status.idle": "2025-02-18T23:32:32.330860Z",
     "shell.execute_reply.started": "2025-02-18T23:32:24.677544Z",
     "shell.execute_reply": "2025-02-18T23:32:32.330184Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-18T01:15:23.315634Z",
     "start_time": "2025-03-18T01:15:20.674447Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data and model"
   ],
   "metadata": {
    "id": "HfSHyQlT-fVF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "qqp = datasets.load_dataset('SetFit/qqp')\n",
    "print('\\n')\n",
    "print(\"Sample[0]:\", qqp['train'][0])\n",
    "print(\"Sample[3]:\", qqp['train'][3])"
   ],
   "metadata": {
    "id": "Y2_wgtrx8e6C",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T23:32:32.331770Z",
     "iopub.execute_input": "2025-02-18T23:32:32.332114Z",
     "iopub.status.idle": "2025-02-18T23:32:37.538516Z",
     "shell.execute_reply.started": "2025-02-18T23:32:32.332094Z",
     "shell.execute_reply": "2025-02-18T23:32:37.537884Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-18T01:15:24.282888Z",
     "start_time": "2025-03-18T01:15:23.322313Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sample[0]: {'text1': 'How is the life of a math student? Could you describe your own experiences?', 'text2': 'Which level of prepration is enough for the exam jlpt5?', 'label': 0, 'idx': 0, 'label_text': 'not duplicate'}\n",
      "Sample[3]: {'text1': 'What can one do after MBBS?', 'text2': 'What do i do after my MBBS ?', 'label': 1, 'idx': 3, 'label_text': 'duplicate'}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"gchhablani/bert-base-cased-finetuned-qqp\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
   ],
   "metadata": {
    "id": "pStlWcvD8rdk",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T19:38:21.326118Z",
     "iopub.execute_input": "2025-02-18T19:38:21.326646Z",
     "iopub.status.idle": "2025-02-18T19:38:41.323030Z",
     "shell.execute_reply.started": "2025-02-18T19:38:21.326615Z",
     "shell.execute_reply": "2025-02-18T19:38:41.321612Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-18T01:15:26.896028Z",
     "start_time": "2025-03-18T01:15:24.460233Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize the data"
   ],
   "metadata": {
    "id": "hM3ZujeZ-Z7E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "MAX_LENGTH = 128\n",
    "def preprocess_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples['text1'], examples['text2'],\n",
    "        padding='max_length', max_length=MAX_LENGTH, truncation=True\n",
    "    )\n",
    "    result['label'] = examples['label']\n",
    "    return result\n",
    "\n",
    "qqp_preprocessed = qqp.map(preprocess_function, batched=True)"
   ],
   "metadata": {
    "id": "qtkllSPG9bTL",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T19:38:49.441562Z",
     "iopub.execute_input": "2025-02-18T19:38:49.442588Z",
     "iopub.status.idle": "2025-02-18T19:40:24.278815Z",
     "shell.execute_reply.started": "2025-02-18T19:38:49.442558Z",
     "shell.execute_reply": "2025-02-18T19:40:24.278038Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-18T01:16:00.671942Z",
     "start_time": "2025-03-18T01:15:26.908266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/390965 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3063081d1b544c29ae4e7c5dd7afddfa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "print(repr(qqp_preprocessed['train'][0]['input_ids'])[:100], \"...\")"
   ],
   "metadata": {
    "id": "ObMcFN59_Ll2",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T19:40:24.279984Z",
     "iopub.execute_input": "2025-02-18T19:40:24.280305Z",
     "iopub.status.idle": "2025-02-18T19:40:24.287423Z",
     "shell.execute_reply.started": "2025-02-18T19:40:24.280270Z",
     "shell.execute_reply": "2025-02-18T19:40:24.286529Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-18T01:16:00.688866Z",
     "start_time": "2025-03-18T01:16:00.685049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1731, 1110, 1103, 1297, 1104, 170, 12523, 2377, 136, 7426, 1128, 5594, 1240, 1319, 5758, 136,  ...\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "print(tokenizer.decode(qqp_preprocessed['train'][0]['input_ids']))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T19:40:24.288839Z",
     "iopub.execute_input": "2025-02-18T19:40:24.289075Z",
     "iopub.status.idle": "2025-02-18T19:40:24.658139Z",
     "shell.execute_reply.started": "2025-02-18T19:40:24.289054Z",
     "shell.execute_reply": "2025-02-18T19:40:24.657150Z"
    },
    "id": "OV0i_trzGxLA",
    "ExecuteTime": {
     "end_time": "2025-03-18T01:16:00.754635Z",
     "start_time": "2025-03-18T01:16:00.727476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] How is the life of a math student? Could you describe your own experiences? [SEP] Which level of prepration is enough for the exam jlpt5? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1: evaluation (3 point)\n",
    "\n",
    "We randomly chose a model trained on QQP - but is it any good?\n",
    "\n",
    "One way to assess this is by measuring validation accuracy, which you will implement next.\n",
    "\n",
    "Hereâ€™s the interface to help you get started:"
   ],
   "metadata": {
    "id": "PyQ1ZbzGAUF2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "val_set = qqp_preprocessed['validation']\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=12, shuffle=False, collate_fn=transformers.default_data_collator, num_workers=4\n",
    ")"
   ],
   "metadata": {
    "id": "M5ueSoieAbBg",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T23:32:13.981271Z",
     "iopub.execute_input": "2025-02-18T23:32:13.981563Z",
     "iopub.status.idle": "2025-02-18T23:32:13.990865Z",
     "shell.execute_reply.started": "2025-02-18T23:32:13.981538Z",
     "shell.execute_reply": "2025-02-18T23:32:13.989774Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-18T01:16:00.894928Z",
     "start_time": "2025-03-18T01:16:00.773660Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "for batch in val_loader:\n",
    "    break  # here will be your training code later. For now, it reads one batch\n",
    "print(\"Sample batch:\", batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "  predicted = model(\n",
    "      input_ids=batch['input_ids'],\n",
    "      attention_mask=batch['attention_mask'],\n",
    "      token_type_ids=batch['token_type_ids']\n",
    "  )\n",
    "\n",
    "print('\\nPrediction (probs):', torch.softmax(predicted.logits, dim=1).data.numpy())"
   ],
   "metadata": {
    "id": "SsPwXXx-At-i",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T19:40:36.760446Z",
     "iopub.execute_input": "2025-02-18T19:40:36.760770Z",
     "iopub.status.idle": "2025-02-18T19:40:51.373736Z",
     "shell.execute_reply.started": "2025-02-18T19:40:36.760747Z",
     "shell.execute_reply": "2025-02-18T19:40:51.372727Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-18T01:16:01.814389Z",
     "start_time": "2025-03-18T01:16:00.907785Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch: {'labels': tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0]), 'idx': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), 'input_ids': tensor([[ 101, 2009, 1132,  ...,    0,    0,    0],\n",
      "        [ 101,  146, 1328,  ...,    0,    0,    0],\n",
      "        [ 101, 2181, 1175,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2009, 1431,  ...,    0,    0,    0],\n",
      "        [ 101, 2825, 1128,  ...,    0,    0,    0],\n",
      "        [ 101, 2825, 2256,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction (probs): [[9.9989271e-01 1.0728100e-04]\n",
      " [9.9997532e-01 2.4653616e-05]\n",
      " [9.2760995e-03 9.9072391e-01]\n",
      " [9.9996829e-01 3.1664582e-05]\n",
      " [9.9684072e-01 3.1592713e-03]\n",
      " [9.1056442e-03 9.9089432e-01]\n",
      " [9.9436378e-01 5.6362618e-03]\n",
      " [9.2441821e-01 7.5581826e-02]\n",
      " [4.5977063e-02 9.5402288e-01]\n",
      " [9.9997604e-01 2.3962646e-05]\n",
      " [9.9003965e-01 9.9603338e-03]\n",
      " [9.9997747e-01 2.2565942e-05]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Your task__ is to measure the validation accuracy of your model.\n",
    "Doing so naively may take several hours. Please make sure you use the following optimizations:\n",
    "\n",
    "- run the model on GPU with no_grad\n",
    "- using batch size larger than 1\n",
    "- use optimize data loader with num_workers > 1\n",
    "- (optional) use [mixed precision](https://pytorch.org/docs/stable/notes/amp_examples.html)\n"
   ],
   "metadata": {
    "id": "RoxHzxn0DQqO"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T19:41:19.390240Z",
     "iopub.execute_input": "2025-02-18T19:41:19.390609Z",
     "iopub.status.idle": "2025-02-18T19:41:19.394631Z",
     "shell.execute_reply.started": "2025-02-18T19:41:19.390571Z",
     "shell.execute_reply": "2025-02-18T19:41:19.393619Z"
    },
    "id": "CV4EfZpJGxLA",
    "ExecuteTime": {
     "end_time": "2025-03-18T01:16:01.834707Z",
     "start_time": "2025-03-18T01:16:01.832899Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T19:41:19.662889Z",
     "iopub.execute_input": "2025-02-18T19:41:19.663179Z",
     "iopub.status.idle": "2025-02-18T19:41:20.062483Z",
     "shell.execute_reply.started": "2025-02-18T19:41:19.663156Z",
     "shell.execute_reply": "2025-02-18T19:41:20.061582Z"
    },
    "id": "rVisozXgGxLA",
    "ExecuteTime": {
     "end_time": "2025-03-18T01:16:01.877424Z",
     "start_time": "2025-03-18T01:16:01.875432Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T23:33:03.019153Z",
     "iopub.execute_input": "2025-02-18T23:33:03.019619Z",
     "iopub.status.idle": "2025-02-18T23:33:03.023118Z",
     "shell.execute_reply.started": "2025-02-18T23:33:03.019595Z",
     "shell.execute_reply": "2025-02-18T23:33:03.022433Z"
    },
    "id": "3dodymkdGxLA",
    "ExecuteTime": {
     "end_time": "2025-03-18T01:16:01.922230Z",
     "start_time": "2025-03-18T01:16:01.920337Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "# Load the validation dataset\n",
    "val_set = qqp_preprocessed['validation']\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=12, shuffle=False, collate_fn=transformers.default_data_collator, num_workers=4\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n"
   ],
   "metadata": {
    "id": "9k5EK7-KA5F2",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T05:16:03.757052Z",
     "iopub.execute_input": "2025-02-18T05:16:03.757300Z",
     "iopub.status.idle": "2025-02-18T05:16:03.762556Z",
     "shell.execute_reply.started": "2025-02-18T05:16:03.757278Z",
     "shell.execute_reply": "2025-02-18T05:16:03.761476Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-18T01:17:17.400221Z",
     "start_time": "2025-03-18T01:16:01.966212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3370 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a36a212934c54edfa04a063dde76632e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9084\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "assert 0.9 < accuracy < 0.91"
   ],
   "metadata": {
    "id": "0R2z_-FZU3qy",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T05:16:16.517878Z",
     "iopub.execute_input": "2025-02-18T05:16:16.518197Z",
     "iopub.status.idle": "2025-02-18T05:16:16.522121Z",
     "shell.execute_reply.started": "2025-02-18T05:16:16.518175Z",
     "shell.execute_reply": "2025-02-18T05:16:16.521334Z"
    },
    "ExecuteTime": {
     "end_time": "2025-03-18T01:17:17.549879Z",
     "start_time": "2025-03-18T01:17:17.546953Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2: train the model (5 points)\n",
    "\n",
    "Fine-tune your own model. You are free to choose any model __except for the original BERT.__ We recommend [DeBERTa-v3](https://huggingface.co/microsoft/deberta-v3-base), but you can choose the best model based on public benchmarks (e.g. [GLUE](https://gluebenchmark.com/)).\n",
    "\n",
    "You can write the training code manually (as we did in class) or use transformers.Trainer (see [this example](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification)). Please make sure that your model's accuracy is at least __comparable__ with the above example for BERT."
   ],
   "metadata": {
    "id": "KONQ1E0J-y6B"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = transformers.DebertaV2Tokenizer.from_pretrained(model_name)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-18T23:33:09.217189Z",
     "iopub.execute_input": "2025-02-18T23:33:09.217494Z",
     "iopub.status.idle": "2025-02-18T23:33:38.259309Z",
     "shell.execute_reply.started": "2025-02-18T23:33:09.217466Z",
     "shell.execute_reply": "2025-02-18T23:33:38.258204Z"
    },
    "id": "Sls1fB9DGxLB",
    "ExecuteTime": {
     "end_time": "2025-03-18T01:19:08.965613Z",
     "start_time": "2025-03-18T01:19:05.003611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7aa7e303c545463aa45f7d6a99681477"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "id": "T0ZkZTkl_yMU",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-18T01:22:43.050133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the training dataset\n",
    "train_set = qqp_preprocessed['train']\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=12, shuffle=True, collate_fn=transformers.default_data_collator, num_workers=4\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model and save the model after each epoch\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate the model\n",
    "\n",
    "        if total_predictions % 100 == 0:\n",
    "            model.eval()\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    token_type_ids = batch['token_type_ids'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                    correct_predictions += (predictions == labels).sum().item()\n",
    "                    total_predictions += labels.size(0)\n",
    "\n",
    "            accuracy = correct_predictions / total_predictions\n",
    "            print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "            # check if the model is accurate enough between 0.89 and 0.92\n",
    "            if 0.89 < accuracy < 0.92:\n",
    "                break\n",
    "\n",
    "            model.train()\n",
    "\n",
    "    torch.save(model.state_dict(), f'model_epoch_{epoch}.pt')\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/30321 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4915f7e71964828a40f50aace05972a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true,
    "id": "GhVwWPwSGxLC"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 3: try the full pipeline (2 point)\n",
    "\n",
    "Finally, it is time to use your model to find duplicate questions.\n",
    "Please implement a function that takes a question and finds top-5 potential duplicates in the training set. For now, it is fine if your function is slow, as long as it yields correct results.\n",
    "\n",
    "Showcase how your function works with at least 3 examples."
   ],
   "metadata": {
    "id": "wQD0IV44LrSs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "zLSjmsKaUyQb"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
